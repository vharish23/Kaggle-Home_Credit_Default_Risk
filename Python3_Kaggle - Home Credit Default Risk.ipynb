{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Include if using datalab\n",
    "# !pip3 install lightgbm\n",
    "# !pip3 install tqdm\n",
    "# !pip3 install shap\n",
    "# import google.datalab.storage as storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import scipy\n",
    "from sklearn.preprocessing import LabelEncoder, Imputer, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter('ignore', UserWarning)\n",
    "\n",
    "import gc\n",
    "gc.enable()\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "%matplotlib inline\n",
    "\n",
    "#Open solution\n",
    "import sys\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from functools import partial\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print all rows and columns. Dont hide any\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['actual_importances_ditribution_rf.csv', 'submission_XL.csv', 'fm_sub_preds_1.npy', 'submission_LGB_Clf_nullimportance_ExtMeanImputed.csv', 'submission_LGB_Clf_BO_prev_imp.csv', 'application_test.csv', 'submission_LGB_Clf_nullimportance_BayOpt.csv', 'xgb_importances01.png', '.DS_Store', 'bureau_balance.csv.zip', 'pos_utils.py', 'application_train.csv.zip', 'fm_oof_preds_1.npy', 'submission_LGB_Clf_nullimportance.csv', 'credit_card_balance.csv.zip', 'Test', 'null_importances_distribution_rf.csv', 'application_test.csv.zip', 'submission_LGB_Clf_PCA_null_gain_gt_0.csv', 'lgbm_importances01.png', 'submission_LGB_Clf_BO_prev_imp_LR_0.009.csv', 'HomeCredit_imp_features.png', 'lgb_oof_preds_null_gain_gt_0.csv', 'HomeCredit_columns_description.csv', 'POS_CASH_balance.csv', 'submission_LGB_Clf_newBuro.csv', 'Code', 'submission_LGB_Clf_BO_new.csv', 'Rankavg_blend_voting_stack3.csv', 'credit_card_balance.csv', 'HomeCredit_columns_description.xlsx', 'installments_payments.csv', 'application_train.csv', 'Oof_files', 'installments_payments.csv.zip', 'submission_LGB_Clf_null_importance.csv', 'Blend', 'submission_Voting_missing_corrected_80309_CV.csv', 'bureau.csv', 'Ext_Source', 'POS_CASH_balance.csv.zip', 'submission_XGB_null.csv', 'submission_LGB_Clf_imputed_800.csv', 'lgbm_importances_53.png', 'submission_LGB_Clf_CCnewPCA.csv', 'IJARCET-VOL-5-ISSUE-3-705-718.pdf', 'submission_LGB_Clf_ImputationChange.csv', 'HC_AltRun_notebook_env.db', 'submission_LGB_Clf_null_pos_ins.csv', 'submission_LGB_Clf_First_Ins_Trend.csv', 'submission_LGB_Clf_prev_Appr_Ref_PCA_801.csv', 'open-solution-home-credit-master', 'submission_LGB_Clf_NewBur_801.csv', 'lgb_oof_preds_GP.csv', 'HC_null_importance_notebook_env.db', 'previous_application.csv', 'HC_shap_notebook_env.db', 'zipped', 'bureau.csv.zip', 'bureau_balance.csv', 'master_train_test.csv', 'previous_application.csv.zip', 'sample_submission.csv.zip', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir('../Documents/JK/Home_Credit_Default_Risk'))\n",
    "dir = '../Documents/JK/Home_Credit_Default_Risk'\n",
    "del os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Path to the object in Google Cloud Storage that you want to copy\n",
    "# sample_gcs_object = 'gs://hcdr-fast2/HC.zip'\n",
    "\n",
    "# # Copy the file from Google Cloud Storage to Datalab\n",
    "# !gsutil cp $sample_gcs_object 'data/HC.zip'\n",
    "\n",
    "# print(\"Data Copied..\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Extract zip files\n",
    "# import zipfile\n",
    "# with zipfile.ZipFile(\"data/HC.zip\",\"r\") as zip_ref:\n",
    "#     zip_ref.extractall(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To save notebook session to continue later\n",
    "import dill\n",
    "# dill.dump_session('HC_AltRun_notebook_env.db')\n",
    "dill.load_session(dir+'HC_AltRun_notebook_env.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to creating dummy variables for Nominal fields for a set of columns\n",
    "def create_dummy_set(df_col,df):\n",
    "    for i in df_col:\n",
    "        dummies = pd.get_dummies(df[i]).rename(columns=lambda x: i+'_'+ str(x))\n",
    "        #Adding to input variables\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "        #Dropping column without having to reassign\n",
    "        df.drop([i], inplace=True, axis=1)\n",
    "        print (df.shape)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To handle binary variables\n",
    "lb=LabelEncoder()\n",
    "def LabelEncoding_Cat(df):\n",
    "    df=df.copy()\n",
    "    Cat_Var=df.select_dtypes(include=['object']).columns.tolist()\n",
    "    for col in Cat_Var:\n",
    "        df[col]=lb.fit_transform(df[col].astype('str'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category = False):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object' or df[col].dtype.name == 'category']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to check % of missing values\n",
    "def missing_values_table(df): \n",
    "    mis_val = df.isnull().sum()\n",
    "    mis_val_percent = 100 * df.isnull().sum()/len(df)\n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "    return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to eliminate variables with more than 95% missing values - need function 'missing_values_table'\n",
    "def non_missing_col(df):\n",
    "    df_col = missing_values_table(df) #Calc missing values for each column\n",
    "    df_col = df_col.loc[df_col['% of Total Values'] <= 95] #Filter dataframe with less missing values\n",
    "    df_col = df_col.index.values.tolist() #Convert dataframe to columns list\n",
    "    return df_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to change the dataframe memory/size to optimal value\n",
    "def reduce_mem_usage(data, verbose = True):\n",
    "    start_mem = data.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Memory usage of dataframe: {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in data.columns:\n",
    "        col_type = data[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = data[col].min()\n",
    "            c_max = data[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    data[col] = data[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    data[col] = data[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    data[col] = data[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    data[col] = data[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    data[col] = data[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    data[col] = data[col].astype(np.float32)\n",
    "                else:\n",
    "                    data[col] = data[col].astype(np.float64)\n",
    "\n",
    "    end_mem = data.memory_usage().sum() / 1024**2\n",
    "    if verbose:\n",
    "        print('Memory usage after optimization: {:.2f} MB'.format(end_mem))\n",
    "        print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to use from open solutions - Thanks to https://neptune.ml/\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import multiprocessing as mp\n",
    "from functools import reduce\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "# import yaml\n",
    "# from attrdict import AttrDict\n",
    "\n",
    "def add_features(feature_name, aggs, features, feature_names, groupby):\n",
    "    feature_names.extend(['{}_{}'.format(feature_name, agg) for agg in aggs])\n",
    "\n",
    "    for agg in aggs:\n",
    "        if agg == 'kurt':\n",
    "            agg_func = kurtosis\n",
    "        elif agg == 'iqr':\n",
    "            agg_func = iqr\n",
    "        else:\n",
    "            agg_func = agg\n",
    "        \n",
    "        g = groupby[feature_name].agg(agg_func).reset_index().rename(index=str,\n",
    "                                                                columns={feature_name: '{}_{}'.format(feature_name,\n",
    "                                                                                                      agg)})\n",
    "        features = features.merge(g, on='SK_ID_CURR', how='left')\n",
    "    return features, feature_names\n",
    "\n",
    "def add_features_in_group(features, gr_, feature_name, aggs, prefix):\n",
    "    for agg in aggs:\n",
    "        if agg == 'sum':\n",
    "            features['{}{}_sum'.format(prefix, feature_name)] = gr_[feature_name].sum()\n",
    "        elif agg == 'mean':\n",
    "            features['{}{}_mean'.format(prefix, feature_name)] = gr_[feature_name].mean()\n",
    "        elif agg == 'max':\n",
    "            features['{}{}_max'.format(prefix, feature_name)] = gr_[feature_name].max()\n",
    "        elif agg == 'min':\n",
    "            features['{}{}_min'.format(prefix, feature_name)] = gr_[feature_name].min()\n",
    "        elif agg == 'std':\n",
    "            features['{}{}_std'.format(prefix, feature_name)] = gr_[feature_name].std()\n",
    "        elif agg == 'count':\n",
    "            features['{}{}_count'.format(prefix, feature_name)] = gr_[feature_name].count()\n",
    "        elif agg == 'skew':\n",
    "            features['{}{}_skew'.format(prefix, feature_name)] = skew(gr_[feature_name])\n",
    "        elif agg == 'kurt':\n",
    "            features['{}{}_kurt'.format(prefix, feature_name)] = kurtosis(gr_[feature_name])\n",
    "        elif agg == 'iqr':\n",
    "            features['{}{}_iqr'.format(prefix, feature_name)] = iqr(gr_[feature_name])\n",
    "        elif agg == 'median':\n",
    "            features['{}{}_median'.format(prefix, feature_name)] = gr_[feature_name].median()\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def calculate_rank(predictions):\n",
    "    rank = (1 + predictions.rank().values) / (predictions.shape[0] + 1)\n",
    "    return rank\n",
    "\n",
    "\n",
    "def chunk_groups(groupby_object, chunk_size):\n",
    "    n_groups = groupby_object.ngroups\n",
    "    group_chunk, index_chunk = [], []\n",
    "    for i, (index, df) in enumerate(groupby_object):\n",
    "        group_chunk.append(df)\n",
    "        index_chunk.append(index)\n",
    "\n",
    "        if (i + 1) % chunk_size == 0 or i + 1 == n_groups:\n",
    "            group_chunk_, index_chunk_ = group_chunk.copy(), index_chunk.copy()\n",
    "            group_chunk, index_chunk = [], []\n",
    "            yield index_chunk_, group_chunk_\n",
    "\n",
    "\n",
    "def parallel_apply(groups, func, index_name='Index', num_workers=1, chunk_size=100000):\n",
    "    n_chunks = np.ceil(1.0 * groups.ngroups / chunk_size)\n",
    "    indeces, features = [], []\n",
    "    for index_chunk, groups_chunk in tqdm(chunk_groups(groups, chunk_size), total=n_chunks):\n",
    "        with mp.pool.Pool(num_workers) as executor:\n",
    "            features_chunk = executor.map(func, groups_chunk)\n",
    "        features.extend(features_chunk)\n",
    "        indeces.extend(index_chunk)\n",
    "\n",
    "    features = pd.DataFrame(features)\n",
    "    features.index = indeces\n",
    "    features.index.name = index_name\n",
    "    return features\n",
    "\n",
    "def last_k_installment_features(gr, periods):\n",
    "    gr_ = gr.copy()\n",
    "    gr_.sort_values(['MONTHS_BALANCE'], ascending=False, inplace=True)\n",
    "\n",
    "    features = {}\n",
    "    for period in periods:\n",
    "        if period > 10e10:\n",
    "            period_name = 'ALL_INSTALLMENT_'\n",
    "            gr_period = gr_.copy()\n",
    "        else:\n",
    "            period_name = 'LAST_{}_'.format(period)\n",
    "            gr_period = gr_.iloc[:period]\n",
    "\n",
    "        features = add_features_in_group(features, gr_period, 'POS_CASH_PAID_LATE',\n",
    "                                             ['count', 'mean'],\n",
    "                                             period_name)\n",
    "        features = add_features_in_group(features, gr_period, 'POS_CASH_PAID_LATE_WITH_TOLERANCE',\n",
    "                                             ['mean'],\n",
    "                                             period_name)\n",
    "        features = add_features_in_group(features, gr_period, 'SK_DPD',\n",
    "                                             ['sum', 'mean', 'max', 'min'],\n",
    "                                             period_name)\n",
    "        features = add_features_in_group(features, gr_period, 'SK_DPD_DEF',\n",
    "                                             ['sum', 'mean', 'max', 'min'],\n",
    "                                             period_name)\n",
    "    return features\n",
    "\n",
    "def last_loan_features(gr):\n",
    "    gr_ = gr.copy()\n",
    "    gr_.sort_values(['MONTHS_BALANCE'], ascending=False, inplace=True)\n",
    "    last_installment_id = gr_['SK_ID_PREV'].iloc[0]\n",
    "    gr_ = gr_[gr_['SK_ID_PREV'] == last_installment_id]\n",
    "\n",
    "    features={}\n",
    "    features = add_features_in_group(features, gr_, 'POS_CASH_PAID_LATE',\n",
    "                                         ['count', 'sum', 'mean'],\n",
    "                                         'LAST_LOAN_')\n",
    "    features = add_features_in_group(features, gr_, 'POS_CASH_PAID_LATE_WITH_TOLERANCE',\n",
    "                                         ['sum', 'mean'],\n",
    "                                         'LAST_LOAN_')\n",
    "    features = add_features_in_group(features, gr_, 'SK_DPD',\n",
    "                                         ['sum', 'mean', 'max', 'min', 'std'],\n",
    "                                         'LAST_LOAN_')\n",
    "    features = add_features_in_group(features, gr_, 'SK_DPD_DEF',\n",
    "                                         ['sum', 'mean', 'max', 'min', 'std'],\n",
    "                                         'LAST_LOAN_')\n",
    "    return features\n",
    "\n",
    "def trend_in_last_k_installment_features(gr, periods):\n",
    "    gr_ = gr.copy()\n",
    "    gr_.sort_values(['MONTHS_BALANCE'], ascending=False, inplace=True)\n",
    "\n",
    "    features = {}\n",
    "    for period in periods:\n",
    "        gr_period = gr_.iloc[:period]\n",
    "\n",
    "        features = add_trend_feature(features, gr_period,\n",
    "                                         'SK_DPD', '{}_PERIOD_TREND_'.format(period)\n",
    "                                         )\n",
    "        features = add_trend_feature(features, gr_period,\n",
    "                                         'SK_DPD_DEF', '{}_PERIOD_TREND_'.format(period)\n",
    "                                         )\n",
    "    return features\n",
    "\n",
    "def add_trend_feature(features, gr, feature_name, prefix):\n",
    "    y = gr[feature_name].values\n",
    "    try:\n",
    "        x = np.arange(0, len(y)).reshape(-1, 1)\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(x, y)\n",
    "        trend = lr.coef_[0]\n",
    "    except:\n",
    "        trend = np.nan\n",
    "    features['{}{}'.format(prefix, feature_name)] = trend\n",
    "    return features\n",
    "\n",
    "def trend_in_last_k_instalment_features(gr, periods):\n",
    "    gr_ = gr.copy()\n",
    "    gr_.sort_values(['DAYS_INSTALMENT'],ascending=False, inplace=True)\n",
    "    \n",
    "    features = {}\n",
    "\n",
    "    for period in periods:\n",
    "        gr_period = gr_.iloc[:period]\n",
    "\n",
    "\n",
    "        features = _add_trend_feature(features,gr_period,\n",
    "                                      'INST_PAID_LATE_IN_DAYS','{}_PERIOD_TREND_'.format(period)\n",
    "                                     )\n",
    "        features = _add_trend_feature(features,gr_period,\n",
    "                                      'INST_PAID_OVER_AMT','{}_PERIOD_TREND_'.format(period)\n",
    "                                     )\n",
    "    return features\n",
    "\n",
    "def _add_trend_feature(features,gr,feature_name, prefix):\n",
    "    y = gr[feature_name].values\n",
    "    try:\n",
    "        x = np.arange(0,len(y)).reshape(-1,1)\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(x,y)\n",
    "        trend = lr.coef_[0]\n",
    "    except:\n",
    "        trend=np.nan\n",
    "    features['{}{}'.format(prefix,feature_name)] = trend\n",
    "    return features\n",
    "\n",
    "def last_k_instalment_features_with_fractions(gr, periods, fraction_periods):\n",
    "    gr_ = gr.copy()\n",
    "    gr_.sort_values(['DAYS_INSTALMENT'],ascending=False, inplace=True)\n",
    "    \n",
    "    features = {}\n",
    "\n",
    "    for period in periods:\n",
    "        gr_period = gr_.iloc[:period]\n",
    "   \n",
    "        features = add_features_in_group(features,gr_period, 'NUM_INSTALMENT_VERSION', \n",
    "                                       ['sum','mean','max','min','std', 'median'],\n",
    "                                         'LAST_{}_'.format(period))\n",
    "        \n",
    "        features = add_features_in_group(features,gr_period, 'INST_PAID_LATE_IN_DAYS', \n",
    "                                       ['sum','mean','max','min','std', 'median'],\n",
    "                                         'LAST_{}_'.format(period))\n",
    "        features = add_features_in_group(features,gr_period ,'INST_PAID_LATE', \n",
    "                                       ['count','mean'],\n",
    "                                         'LAST_{}_'.format(period))\n",
    "        features = add_features_in_group(features,gr_period ,'INST_PAID_OVER_AMT', \n",
    "                                       ['sum','mean','max','min','std', 'median'],\n",
    "                                         'LAST_{}_'.format(period))\n",
    "        features = add_features_in_group(features,gr_period,'INST_PAID_OVER', \n",
    "                                       ['mean'],\n",
    "                                         'LAST_{}_'.format(period))     \n",
    "    \n",
    "    \n",
    "    for short_period, long_period in fraction_periods:\n",
    "        short_feature_names = _get_feature_names(features, short_period)\n",
    "        long_feature_names = _get_feature_names(features, long_period)\n",
    "        \n",
    "        for short_feature, long_feature in zip(short_feature_names, long_feature_names):\n",
    "            old_name_chunk = '_{}_'.format(short_period)\n",
    "            new_name_chunk ='_{}by{}_fraction_'.format(short_period, long_period)\n",
    "            fraction_feature_name = short_feature.replace(old_name_chunk, new_name_chunk)\n",
    "            features[fraction_feature_name] = safe_div(features[short_feature], features[long_feature])\n",
    "    return pd.Series(features)\n",
    "\n",
    "def _get_feature_names(features, period):\n",
    "    return sorted([feat for feat in features.keys() if '_{}_'.format(period) in feat])\n",
    "\n",
    "def safe_div(a,b):\n",
    "    try:\n",
    "        return float(a)/float(b)\n",
    "    except:\n",
    "        return 0.0\n",
    "    \n",
    "def aggregate_and_find_diff(df,AGGREGATION_RECIPIES):\n",
    "    for groupby_cols, specs in AGGREGATION_RECIPIES:\n",
    "        group_object = df.groupby(groupby_cols)\n",
    "        for select, agg in specs:\n",
    "            groupby_aggregate_name = '{}_{}_{}'.format('_'.join(groupby_cols), agg, select)\n",
    "            df = df.merge(group_object[select]\n",
    "                              .agg(agg)\n",
    "                              .reset_index()\n",
    "                              .rename(columns={select: groupby_aggregate_name})\n",
    "                              [groupby_cols + [groupby_aggregate_name]],\n",
    "                              on=groupby_cols,\n",
    "                              how='left')\n",
    "            if agg in ['mean','median','max','min']:\n",
    "                diff_name = '{}_diff'.format(groupby_aggregate_name)\n",
    "                abs_diff_name = '{}_abs_diff'.format(groupby_aggregate_name)\n",
    "\n",
    "                df[diff_name] = df[select] - df[groupby_aggregate_name] \n",
    "                df[abs_diff_name] = np.abs(df[select] - df[groupby_aggregate_name]) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data....\n",
      "\n",
      "Memory usage of dataframe: 471.48 MB\n",
      "Memory usage after optimization: 309.01 MB\n",
      "Decreased by 34.5%\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data....\\n\")\n",
    "prev = pd.read_csv(dir+'previous_application.csv')\n",
    "prev = reduce_mem_usage(prev, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess previous_applications.csv\n",
    "#Creating bins of hours\n",
    "bins = [0,11,15,24]\n",
    "prev['HOUR_APPR_PROCESS_START'] = pd.cut(prev['HOUR_APPR_PROCESS_START'],bins, labels=[\"morning\", \"noon\", \"evening\"])\n",
    "#Reducing categories\n",
    "di = {\"MONDAY\" : 1,\"TUESDAY\": 1,\"WEDNESDAY\": 1,\"THURSDAY\": 1,\"FRIDAY\": 1,\n",
    "      \"SATURDAY\" : 0,\"SUNDAY\" : 0}\n",
    "prev['WEEKDAY_APPR_PROCESS_START'] = prev['WEEKDAY_APPR_PROCESS_START'].map(di)\n",
    "prev['NAME_CASH_LOAN_PURPOSE'] = np.where(prev.NAME_CASH_LOAN_PURPOSE.isin(['Building a house or an annex','Buying a garage','Buying a holiday home / land','Buying a home']), 'Real Estate', prev.NAME_CASH_LOAN_PURPOSE)\n",
    "prev['NAME_CASH_LOAN_PURPOSE'].replace(['XAP','XNA'], np.nan, inplace= True)\n",
    "prev['NAME_CASH_LOAN_PURPOSE'] = np.where(~prev.NAME_CASH_LOAN_PURPOSE.isin(['Real Estate','Payments on other loans','Repairs',np.nan]),'Others', prev.NAME_CASH_LOAN_PURPOSE)\n",
    "prev['NAME_TYPE_SUITE'] = np.where(prev.NAME_TYPE_SUITE.isin(['Children','Spouse, partner','Group of people','Family']), 'Accompanied', prev.NAME_TYPE_SUITE)\n",
    "prev['NAME_SELLER_INDUSTRY'] = np.where(prev.NAME_SELLER_INDUSTRY.isin(['Jewelry','Clothing','Furniture']), 'Fashion and High Maintenance', prev.NAME_SELLER_INDUSTRY)\n",
    "prev['NAME_SELLER_INDUSTRY'] = np.where(prev.NAME_SELLER_INDUSTRY.isin(['Auto technology','Connectivity','Consumer electronics']), 'Techie', prev.NAME_SELLER_INDUSTRY)\n",
    "prev['NAME_GOODS_CATEGORY'] = np.where(prev.NAME_GOODS_CATEGORY.isin(['Construction Materials','House Construction']),'House Related', prev.NAME_GOODS_CATEGORY)\n",
    "prev['NAME_GOODS_CATEGORY'] = np.where(~prev.NAME_GOODS_CATEGORY.isin(['House Related','XNA',]),'NOT House Related', prev.NAME_GOODS_CATEGORY)\n",
    "# Convert to nan\n",
    "XNA_col = ['NAME_PAYMENT_TYPE','NAME_CONTRACT_TYPE','CODE_REJECT_REASON','NAME_CLIENT_TYPE','NAME_PRODUCT_TYPE',\n",
    "           'NAME_YIELD_GROUP','NAME_GOODS_CATEGORY','NAME_PORTFOLIO','NAME_SELLER_INDUSTRY']\n",
    "prev[XNA_col] = prev[XNA_col].replace({'XNA':np.nan})\n",
    "prev['CODE_REJECT_REASON'].replace('XAP', np.nan, inplace= True)\n",
    "## OHE\n",
    "prev, cat_cols = one_hot_encoder(prev, nan_as_category= True)\n",
    "# Days 365.243 values -> nan\n",
    "junk_replace = ['DAYS_FIRST_DRAWING','DAYS_FIRST_DUE','DAYS_LAST_DUE_1ST_VERSION','DAYS_LAST_DUE','DAYS_TERMINATION']\n",
    "prev[junk_replace] = prev[junk_replace].replace(365243, np.nan)\n",
    "# Fill nan as 0 or drop\n",
    "prev.update(prev[['CNT_PAYMENT','AMT_ANNUITY','AMT_DOWN_PAYMENT','RATE_DOWN_PAYMENT','AMT_GOODS_PRICE']].fillna(0))\n",
    "prev = prev[np.isfinite(prev['AMT_CREDIT'])]#Just 1 value dropped - AMT_CREDIT\n",
    "prev = prev[prev.SK_ID_PREV != 2061125]\n",
    "# Add feature: value ask / value received percentage\n",
    "prev['LOAN_TO_VALUE'] = prev['AMT_CREDIT'] / (1 + prev['AMT_GOODS_PRICE'])\n",
    "prev['APP_CREDIT_RATIO'] = prev['AMT_APPLICATION'] / (1 + prev['AMT_CREDIT'])\n",
    "prev['AMT_REPAYMENT_MTHLY'] = prev['AMT_CREDIT'] / (0.1 + prev['CNT_PAYMENT'])\n",
    "prev['AMT_REPAYMENT_MTHLY'] = prev['AMT_REPAYMENT_MTHLY'].apply(lambda x: np.log(x+1))\n",
    "prev['LOAN_PERIOD'] = prev['DAYS_LAST_DUE'] - prev['DAYS_FIRST_DUE']\n",
    "prev.update(prev['LOAN_PERIOD'].fillna(prev['DAYS_LAST_DUE_1ST_VERSION'] - prev['DAYS_FIRST_DUE']))\n",
    "prev.update(prev['LOAN_PERIOD'].fillna(0))\n",
    "prev['TERMINATED_BEF_LAST_YR'] = (prev['DAYS_TERMINATION'] < -365).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Previous applications numeric features\n",
    "num_aggregations = {\n",
    "        'SK_ID_PREV': ['nunique'],\n",
    "        'AMT_ANNUITY': ['max','mean','sum'],\n",
    "        'AMT_APPLICATION': ['max', 'mean'],\n",
    "        'AMT_CREDIT': ['max', 'mean','sum'],\n",
    "        'AMT_GOODS_PRICE': ['max', 'mean','sum'],\n",
    "        'AMT_REPAYMENT_MTHLY': ['min', 'max', 'mean','median','sum'],\n",
    "        'AMT_DOWN_PAYMENT': ['max', 'mean','sum'],\n",
    "        'APP_CREDIT_RATIO': ['max', 'mean'],\n",
    "        'LOAN_TO_VALUE': ['min','max','mean'],\n",
    "        'LOAN_PERIOD': ['max','mean','sum'],\n",
    "        'SELLERPLACE_AREA': ['max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': ['max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean','sum'],\n",
    "        'CNT_PAYMENT': ['max','mean','sum',],\n",
    "        'DAYS_FIRST_DRAWING': ['min','max','mean'],\n",
    "        'DAYS_FIRST_DUE': ['min','max','mean'],\n",
    "        'DAYS_LAST_DUE_1ST_VERSION': ['min','max','mean'],\n",
    "        'DAYS_LAST_DUE': ['min','max','mean'],\n",
    "        'DAYS_TERMINATION': ['max','mean'],   \n",
    "        'TERMINATED_BEF_LAST_YR': ['mean']\n",
    "    }\n",
    "# Previous applications categorical features\n",
    "cat_aggregations = {}\n",
    "for cat in cat_cols:\n",
    "    cat_aggregations[cat] = ['mean']\n",
    "    \n",
    "prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations}).reset_index()\n",
    "prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "prev_agg.rename(columns={'PREV_SK_ID_CURR_':'SK_ID_CURR'}, inplace=True)\n",
    "#Drop redundant - 100% corr\n",
    "prev_agg.drop(['PREV_FLAG_LAST_APPL_PER_CONTRACT_N_MEAN'],axis=1, inplace=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(338857, 158)\n"
     ]
    }
   ],
   "source": [
    "features = pd.DataFrame({'SK_ID_CURR': prev['SK_ID_CURR'].unique()})\n",
    "prev_sorted = prev.sort_values(['SK_ID_CURR', 'DAYS_DECISION'])\n",
    "numbers_of_applications = [1,3,5]\n",
    "for number in numbers_of_applications:\n",
    "    prev_tail = prev_sorted.groupby(by=['SK_ID_CURR']).tail(number)\n",
    "\n",
    "    group_object = prev_tail.groupby(by=['SK_ID_CURR'])['LOAN_TO_VALUE'].mean().reset_index()\n",
    "    group_object.rename(index=str, columns={\n",
    "        'LOAN_TO_VALUE': 'PREV_LTV_OF_LAST_{}_CREDITS_MEAN'.format(number)},\n",
    "                        inplace=True)\n",
    "    features = features.merge(group_object, on=['SK_ID_CURR'], how='left')\n",
    "    \n",
    "    group_object = prev_tail.groupby(by=['SK_ID_CURR'])['CNT_PAYMENT'].mean().reset_index()\n",
    "    group_object.rename(index=str, columns={\n",
    "        'CNT_PAYMENT': 'PREV_APP_TERM_OF_LAST_{}_CREDITS_MEAN'.format(number)},\n",
    "                        inplace=True)\n",
    "    features = features.merge(group_object, on=['SK_ID_CURR'], how='left')\n",
    "\n",
    "    group_object = prev_tail.groupby(by=['SK_ID_CURR'])['DAYS_DECISION'].mean().reset_index()\n",
    "    group_object.rename(index=str, columns={\n",
    "        'DAYS_DECISION': 'PREV_APP_DAYS_DECISION_FOR_LAST_{}_CREDITS_MEAN'.format(number)},\n",
    "                        inplace=True)\n",
    "    features = features.merge(group_object, on=['SK_ID_CURR'], how='left')\n",
    "    \n",
    "    group_object = prev_tail.groupby(by=['SK_ID_CURR'])['DAYS_LAST_DUE_1ST_VERSION'].max().reset_index()\n",
    "    group_object.rename(index=str, columns={\n",
    "        'DAYS_LAST_DUE_1ST_VERSION': 'PREV_APP_DAYS_LAST_DUE_1ST_VERSION_FOR_LAST_{}_CREDITS_MAX'.format(number)},\n",
    "                        inplace=True)\n",
    "    features = features.merge(group_object, on=['SK_ID_CURR'], how='left')\n",
    "\n",
    "## Merge data with agg columns\n",
    "prev_agg = prev_agg.merge(features, how='left', on='SK_ID_CURR')\n",
    "print (prev_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Previous Applications: Approved Applications - only numerical features\n",
    "approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations).reset_index()\n",
    "approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "approved_agg.rename(columns={'APPROVED_SK_ID_CURR_':'SK_ID_CURR'}, inplace=True)\n",
    "prev_agg = prev_agg.merge(approved_agg, how='left', on='SK_ID_CURR')\n",
    "# Previous Applications: Refused Applications - only numerical features\n",
    "refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations).reset_index()\n",
    "refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "refused_agg.rename(columns={'REFUSED_SK_ID_CURR_':'SK_ID_CURR'}, inplace=True)\n",
    "prev_agg = prev_agg.merge(refused_agg, how='left', on='SK_ID_CURR')\n",
    "# prev_agg.fillna(0, inplace=True)\n",
    "del refused, refused_agg, approved, approved_agg, prev, XNA_col,junk_replace, di, bins, cat_cols\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute missing values\n",
    "## Fill days columns with a value to distinguish\n",
    "days_cols = [col for col in prev_agg.columns if 'DAYS' in col]\n",
    "prev_agg.update(prev_agg[days_cols].fillna(999))\n",
    "## Fill others - amount and payment with 0\n",
    "prev_agg.update(prev_agg.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe: 494.11 MB\n",
      "Memory usage after optimization: 214.90 MB\n",
      "Decreased by 56.5%\n"
     ]
    }
   ],
   "source": [
    "prev_agg = reduce_mem_usage(prev_agg, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(338857, 266)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\\n\")\n",
    "cred_card_balance = pd.read_csv(dir+'credit_card_balance.csv') \n",
    "# cred_card_balance = reduce_mem_usage(cred_card_balance, verbose=True) - causing overflow warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Preprocessing...\\n\")\n",
    "\n",
    "cred_card_balance['CREDIT_LOAD_RATIO'] = (1 + cred_card_balance['AMT_BALANCE'])/(2 + cred_card_balance['AMT_CREDIT_LIMIT_ACTUAL'])\n",
    "bins = [0,1,31,61,91,121,181,366,3261]\n",
    "cred_card_balance['SK_DPD_BIN'] = pd.cut(cred_card_balance['SK_DPD'],bins, labels=[0,1,2,3,4,5,6,7],right=False).astype('int64')\n",
    "cred_card_balance['SK_DPD_DEF_BIN'] = pd.cut(cred_card_balance['SK_DPD_DEF'],bins, labels=[0,1,2,3,4,5,6,7],right=False).astype('int64')\n",
    "cred_card_balance.drop(['SK_DPD','SK_DPD_DEF'], axis=1, inplace=True)\n",
    "\n",
    "AMT_CNT_NA_COLS = ['AMT_DRAWINGS_ATM_CURRENT','AMT_DRAWINGS_OTHER_CURRENT','AMT_DRAWINGS_POS_CURRENT',\n",
    "                   'CNT_DRAWINGS_ATM_CURRENT','CNT_DRAWINGS_OTHER_CURRENT','CNT_DRAWINGS_POS_CURRENT',\n",
    "                   'AMT_PAYMENT_CURRENT']\n",
    "cred_card_balance.update(cred_card_balance[AMT_CNT_NA_COLS].fillna(0))\n",
    "\n",
    "cred_card_balance['MIN_AMT_PAID_IND'] = ((cred_card_balance['AMT_PAYMENT_CURRENT'] - cred_card_balance['AMT_INST_MIN_REGULARITY']) >= 0).astype(int)\n",
    "cred_card_balance['MIN_AMT_PAID_RATIO'] = (cred_card_balance['AMT_PAYMENT_CURRENT'] / (1 + cred_card_balance['AMT_INST_MIN_REGULARITY'])).pow(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregating...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Aggregating...\\n\")\n",
    "\n",
    "Label=[s+'_'+l for s in cred_card_balance.columns.tolist() if s not in ['SK_ID_PREV','SK_ID_CURR','NAME_CONTRACT_STATUS'] for l in ['count','mean','median','min','max','sum','var']]\n",
    "agg_cred_card_bal=cred_card_balance.groupby(['SK_ID_PREV','SK_ID_CURR'])\\\n",
    "            .agg(['count','mean','median','min','max','sum','var']).groupby(level='SK_ID_CURR')\\\n",
    "            .agg('mean').round(2).reset_index()\n",
    "agg_cred_card_bal.columns=['SK_ID_CURR']+Label\n",
    "\n",
    "agg_cc_loans = cred_card_balance.groupby('SK_ID_CURR').SK_ID_PREV.nunique().to_frame(name='NUM_LOANS').reset_index()\n",
    "\n",
    "### OHE\n",
    "cred_card_balance, cat_cols = one_hot_encoder(cred_card_balance, nan_as_category= False)\n",
    "cat_cols.extend(('SK_ID_CURR','SK_ID_PREV'))\n",
    "Label=[s+'_'+l for s in cat_cols if s not in ['SK_ID_PREV','SK_ID_CURR'] for l in ['mean']]\n",
    "agg_cred_card_bal_cat = cred_card_balance[cat_cols].groupby(['SK_ID_PREV','SK_ID_CURR'])\\\n",
    "            .agg(['mean']).groupby(level='SK_ID_CURR')\\\n",
    "            .agg('mean').round(2).reset_index()\n",
    "agg_cred_card_bal_cat.columns=['SK_ID_CURR']+Label\n",
    "\n",
    "## Merge Numerical and Cat features\n",
    "agg_cred_card_bal = agg_cred_card_bal.merge(agg_cred_card_bal_cat, on='SK_ID_CURR').merge(agg_cc_loans, on='SK_ID_CURR')\n",
    "\n",
    "del agg_cc_loans, cat_cols, AMT_CNT_NA_COLS, agg_cred_card_bal_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(103558, 190)\n"
     ]
    }
   ],
   "source": [
    "features={}\n",
    "features = pd.DataFrame({'SK_ID_CURR': cred_card_balance['SK_ID_CURR'].unique()})\n",
    "cc_sorted = cred_card_balance.sort_values(['SK_ID_CURR','SK_ID_PREV','MONTHS_BALANCE'])\n",
    "number_of_months = [1,3,6,12]\n",
    "for number in number_of_months:\n",
    "    cc_tail = cc_sorted.groupby(by=['SK_ID_CURR','SK_ID_PREV']).tail(number)\n",
    "\n",
    "    group_object = cc_tail.groupby(by=['SK_ID_CURR'])['CREDIT_LOAD_RATIO'].mean().reset_index()\n",
    "    group_object.rename(index=str, columns={\n",
    "        'CREDIT_LOAD_RATIO': 'CREDIT_LOAD_OF_LAST_{}_MTHS_MEAN'.format(number)},\n",
    "                        inplace=True)\n",
    "    features = features.merge(group_object, on=['SK_ID_CURR'], how='left')\n",
    "    \n",
    "    group_object = cc_tail.groupby(by=['SK_ID_CURR'])['CREDIT_LOAD_RATIO'].max().reset_index()\n",
    "    group_object.rename(index=str, columns={\n",
    "        'CREDIT_LOAD_RATIO': 'CREDIT_LOAD_OF_LAST_{}_MTHS_MAX'.format(number)},\n",
    "                        inplace=True)\n",
    "    features = features.merge(group_object, on=['SK_ID_CURR'], how='left')\n",
    "\n",
    "    group_object = cc_tail.groupby(by=['SK_ID_CURR'])['CNT_DRAWINGS_ATM_CURRENT'].mean().reset_index()\n",
    "    group_object.rename(index=str, columns={\n",
    "        'CNT_DRAWINGS_ATM_CURRENT': 'CNT_DRAWINGS_ATM_CURRENT_OF_LAST_{}_MTHS_MEAN'.format(number)},\n",
    "                        inplace=True)\n",
    "    features = features.merge(group_object, on=['SK_ID_CURR'], how='left')\n",
    "    \n",
    "    group_object = cc_tail.groupby(by=['SK_ID_CURR'])['CNT_DRAWINGS_ATM_CURRENT'].max().reset_index()\n",
    "    group_object.rename(index=str, columns={\n",
    "        'CNT_DRAWINGS_ATM_CURRENT': 'CNT_DRAWINGS_ATM_CURRENT_OF_LAST_{}_MTHS_MAX'.format(number)},\n",
    "                        inplace=True)\n",
    "    features = features.merge(group_object, on=['SK_ID_CURR'], how='left')\n",
    "\n",
    "    group_object = cc_tail.groupby(by=['SK_ID_CURR'])['AMT_BALANCE'].max().reset_index()\n",
    "    group_object.rename(index=str, columns={\n",
    "        'AMT_BALANCE': 'AMT_BALANCE_OF_LAST_{}_MTHS_MAX'.format(number)},\n",
    "                        inplace=True)\n",
    "    features = features.merge(group_object, on=['SK_ID_CURR'], how='left')    \n",
    "    \n",
    "    \n",
    "## Merge data with agg columns\n",
    "agg_cred_card_bal = agg_cred_card_bal.merge(features, how='left', on='SK_ID_CURR')\n",
    "print (agg_cred_card_bal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ColsToDrop = ['MONTHS_BALANCE_mean','MONTHS_BALANCE_median','MONTHS_BALANCE_sum','MONTHS_BALANCE_min',\n",
    "              'AMT_BALANCE_count','AMT_BALANCE_sum','AMT_CREDIT_LIMIT_ACTUAL_count','AMT_CREDIT_LIMIT_ACTUAL_sum',\n",
    "              'AMT_DRAWINGS_CURRENT_count','AMT_DRAWINGS_OTHER_CURRENT_count','AMT_DRAWINGS_POS_CURRENT_count',\n",
    "              'AMT_PAYMENT_TOTAL_CURRENT_count','AMT_RECEIVABLE_PRINCIPAL_count','AMT_RECIVABLE_count',\n",
    "              'AMT_TOTAL_RECEIVABLE_count','CNT_INSTALMENT_MATURE_CUM_count','CNT_INSTALMENT_MATURE_CUM_median',\n",
    "              'CNT_INSTALMENT_MATURE_CUM_mean','CNT_INSTALMENT_MATURE_CUM_min','CNT_INSTALMENT_MATURE_CUM_sum',\n",
    "              'CNT_INSTALMENT_MATURE_CUM_var','MIN_AMT_PAID_IND_sum','MIN_AMT_PAID_IND_count',\n",
    "              'MIN_AMT_PAID_IND_median','MIN_AMT_PAID_IND_min','MIN_AMT_PAID_IND_max','MIN_AMT_PAID_IND_var',\n",
    "              'MIN_AMT_PAID_RATIO_sum','MIN_AMT_PAID_RATIO_count','CNT_DRAWINGS_CURRENT_count','CNT_DRAWINGS_OTHER_CURRENT_count',\n",
    "              'CNT_DRAWINGS_POS_CURRENT_count','AMT_DRAWINGS_ATM_CURRENT_count'\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_cred_card_bal.drop(ColsToDrop, axis=1, inplace=True)\n",
    "agg_cred_card_bal.columns = pd.Index(['CC_' + e for e in agg_cred_card_bal.columns.tolist()])\n",
    "agg_cred_card_bal.rename(columns={'CC_SK_ID_CURR':'SK_ID_CURR'}, inplace=True)\n",
    "del cred_card_balance, ColsToDrop, features, group_object, cc_sorted, number_of_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute missing values\n",
    "## Fill vars columns with zero as there is only 1 month data for Nan values\n",
    "var_cols = [col for col in agg_cred_card_bal.columns if '_var' in col]\n",
    "agg_cred_card_bal.update(agg_cred_card_bal[var_cols].fillna(0))\n",
    "del var_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103558, 157)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_cred_card_bal.shape #Calculate the credit card due amount from recent month data(Month: -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe: 124.83 MB\n",
      "Memory usage after optimization: 48.89 MB\n",
      "Decreased by 60.8%\n"
     ]
    }
   ],
   "source": [
    "agg_cred_card_bal = reduce_mem_usage(agg_cred_card_bal, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Memory usage of dataframe: 610.43 MB\n",
      "Memory usage after optimization: 238.45 MB\n",
      "Decreased by 60.9%\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\\n\")\n",
    "pos_cash_balance = pd.read_csv(dir+'POS_CASH_balance.csv')  \n",
    "pos_cash_balance = reduce_mem_usage(pos_cash_balance, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Label=[s+'_'+l for s in ['MONTHS_BALANCE','SK_DPD','SK_DPD_DEF'] for l in ['size','mean','median','max','sum','std']]\n",
    "agg_pos_cash_balance=pos_cash_balance[['SK_ID_PREV','SK_ID_CURR','MONTHS_BALANCE','SK_DPD','SK_DPD_DEF']]\\\n",
    "            .groupby(['SK_ID_PREV','SK_ID_CURR']).agg(['size','mean','median','max','sum','std'])\\\n",
    "            .groupby(level='SK_ID_CURR').agg('mean').round(4).reset_index()\n",
    "agg_pos_cash_balance.columns=['SK_ID_CURR']+Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dropping columns that are not useful / makes no Biz sense\n",
    "ColToDrop = ['MONTHS_BALANCE_std','MONTHS_BALANCE_sum','SK_DPD_size','SK_DPD_sum','SK_DPD_DEF_size','SK_DPD_DEF_sum']\n",
    "agg_pos_cash_balance.drop(ColToDrop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding Actual number of instalments(and Remaining Instalments) as of app date\n",
    "pos_cash_sorted = pos_cash_balance.sort_values(['SK_ID_CURR','SK_ID_PREV','MONTHS_BALANCE'])\n",
    "\n",
    "group_object1 = pos_cash_sorted.groupby(['SK_ID_CURR','SK_ID_PREV'])['CNT_INSTALMENT_FUTURE','NAME_CONTRACT_STATUS'].last().reset_index()\n",
    "group_object1.rename(columns={'CNT_INSTALMENT_FUTURE': 'REM_INSTALMENTS'},inplace=True)\n",
    "group_object1['REM_INSTALMENTS']= np.where(group_object1['NAME_CONTRACT_STATUS']=='Active', group_object1['REM_INSTALMENTS']-1, group_object1['REM_INSTALMENTS'])\n",
    "group_object1.drop('NAME_CONTRACT_STATUS', axis=1, inplace=True)\n",
    "\n",
    "group_object2 = pos_cash_sorted.groupby(['SK_ID_CURR','SK_ID_PREV'])['CNT_INSTALMENT'].first().reset_index()\n",
    "group_object2.rename(columns={'CNT_INSTALMENT': 'CNT_INSTALMENT_INIT'},inplace=True)\n",
    "\n",
    "pos_cash_sorted = pos_cash_balance.loc[pos_cash_balance.NAME_CONTRACT_STATUS=='Active'].sort_values(['SK_ID_CURR','SK_ID_PREV','MONTHS_BALANCE'])\n",
    "\n",
    "group_object3 = pos_cash_sorted.groupby(['SK_ID_CURR','SK_ID_PREV'])['MONTHS_BALANCE'].count().reset_index()\n",
    "group_object3.rename(columns={'MONTHS_BALANCE': 'MAX_MNTHS_PAID'},inplace=True)\n",
    "\n",
    "features = group_object1.merge(group_object2, on=['SK_ID_CURR','SK_ID_PREV'], how = 'left')\\\n",
    "                        .merge(group_object3, on=['SK_ID_CURR','SK_ID_PREV'], how = 'left').reset_index(drop=True)\n",
    "    \n",
    "features['CNT_INSTALMENT_ACT'] = features['REM_INSTALMENTS'] + features['MAX_MNTHS_PAID']\n",
    "# features.drop(['MAX_MNTHS_PAID'], axis=1, inplace=True)\n",
    "    \n",
    "group_object = pos_cash_balance[['SK_ID_CURR','SK_ID_PREV']].drop_duplicates()\n",
    "##Helper column\n",
    "pos_cash_sorted = pos_cash_balance.loc[pos_cash_balance.CNT_INSTALMENT_FUTURE.notnull()].sort_values(['SK_ID_CURR','SK_ID_PREV','MONTHS_BALANCE'])\n",
    "group_object1 = pos_cash_sorted.groupby(['SK_ID_CURR','SK_ID_PREV'])['MONTHS_BALANCE'].first().reset_index()\n",
    "group_object1.rename(columns={'MONTHS_BALANCE': 'MAX_MNTHS_BALANCE'},inplace=True)\n",
    "\n",
    "pos_cash_sorted = pos_cash_balance.sort_values(['SK_ID_CURR','SK_ID_PREV','MONTHS_BALANCE'])\n",
    "group_object11 = pos_cash_sorted.groupby(['SK_ID_CURR','SK_ID_PREV'])['MONTHS_BALANCE'].first().reset_index()\n",
    "group_object11.rename(columns={'MONTHS_BALANCE': 'MAX_MNTHS_BALANCE_FILLNA'},inplace=True)\n",
    "\n",
    "#### Finding the INST_NUM_OF_FIRST_DPD\n",
    "pos_cash_balance_DPD = pos_cash_balance.loc[pos_cash_balance.SK_DPD > 0]\n",
    "pos_cash_sorted = pos_cash_balance_DPD.sort_values(['SK_ID_CURR','SK_ID_PREV','MONTHS_BALANCE'])\n",
    "group_object2 = pos_cash_sorted.groupby(['SK_ID_CURR','SK_ID_PREV'])['MONTHS_BALANCE'].first().reset_index()\n",
    "group_object2.rename(columns={'MONTHS_BALANCE': 'MONTHS_BALANCE_FROM_FIRST_DPD'},inplace=True)\n",
    "#### Finding the INST_NUM_OF_FIRST_DPD_DEF\n",
    "pos_cash_balance_DPD_DEF = pos_cash_balance.loc[pos_cash_balance.SK_DPD_DEF > 0]\n",
    "pos_cash_sorted = pos_cash_balance_DPD_DEF.sort_values(['SK_ID_CURR','SK_ID_PREV','MONTHS_BALANCE'])\n",
    "group_object3 = pos_cash_sorted.groupby(['SK_ID_CURR','SK_ID_PREV'])['MONTHS_BALANCE'].first().reset_index()\n",
    "group_object3.rename(columns={'MONTHS_BALANCE': 'MONTHS_BALANCE_FROM_FIRST_DPD_DEF'},inplace=True)\n",
    "\n",
    "group_object = group_object.merge(group_object1, on=['SK_ID_CURR','SK_ID_PREV'], how='left')\\\n",
    "                           .merge(group_object2, on=['SK_ID_CURR','SK_ID_PREV'], how='left')\\\n",
    "                           .merge(group_object3, on=['SK_ID_CURR','SK_ID_PREV'], how='left').reset_index(drop=True)\n",
    "#Fill NA - MAX MONTHS        \n",
    "group_object['MAX_MNTHS_BALANCE'] = group_object['MAX_MNTHS_BALANCE'].combine_first(group_object11['MAX_MNTHS_BALANCE_FILLNA'])\n",
    "\n",
    "group_object['INST_OF_FIRST_DPD'] = group_object['MONTHS_BALANCE_FROM_FIRST_DPD'] - group_object['MAX_MNTHS_BALANCE']\n",
    "group_object['INST_OF_FIRST_DPD_DEF'] = group_object['MONTHS_BALANCE_FROM_FIRST_DPD_DEF'] - group_object['MAX_MNTHS_BALANCE']\n",
    "group_object.drop(['MAX_MNTHS_BALANCE','MONTHS_BALANCE_FROM_FIRST_DPD','MONTHS_BALANCE_FROM_FIRST_DPD_DEF'], axis=1, inplace=True)\n",
    "##Merge\n",
    "features = features.merge(group_object, on=['SK_ID_CURR','SK_ID_PREV'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "253"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features['FIRST_DPD_PERCENTILE'] =  features['INST_OF_FIRST_DPD']/features['CNT_INSTALMENT_ACT']\n",
    "features['FIRST_DPD_DEF_PERCENTILE'] =  features['INST_OF_FIRST_DPD_DEF']/features['CNT_INSTALMENT_ACT']\n",
    "features['CNT_INST_RATIO'] = features['CNT_INSTALMENT_ACT']/features['CNT_INSTALMENT_INIT']\n",
    "features['REM_INST_RATIO'] = features['REM_INSTALMENTS']/features['CNT_INSTALMENT_ACT']\n",
    "features['PAID_INST_RATIO'] = features['MAX_MNTHS_PAID']/features['CNT_INSTALMENT_ACT']\n",
    "\n",
    "#percentileofscore\n",
    "del pos_cash_sorted, group_object11, group_object1, group_object2, group_object3, group_object, pos_cash_balance_DPD, pos_cash_balance_DPD_DEF\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_CASH_BALANCE_AGGREGATION_RECIPIES  = [(['SK_ID_CURR'],\n",
    "  [('REM_INSTALMENTS', 'mean'),\n",
    "   ('REM_INSTALMENTS', 'sum'),\n",
    "   ('REM_INSTALMENTS', 'max'),\n",
    "   ('REM_INSTALMENTS', 'median'),\n",
    "   ('CNT_INSTALMENT_ACT', 'mean'),\n",
    "   ('CNT_INSTALMENT_ACT', 'sum'),\n",
    "   ('CNT_INSTALMENT_ACT', 'max'),\n",
    "   ('CNT_INSTALMENT_ACT', 'min'),\n",
    "   ('CNT_INSTALMENT_ACT', 'median'),\n",
    "   ('CNT_INSTALMENT_INIT', 'mean'),\n",
    "   ('CNT_INSTALMENT_INIT', 'sum'),\n",
    "   ('CNT_INSTALMENT_INIT', 'max'),\n",
    "   ('CNT_INSTALMENT_INIT', 'min'),\n",
    "   ('CNT_INSTALMENT_INIT', 'median'),\n",
    "   ('CNT_INST_RATIO', 'mean'),\n",
    "   ('CNT_INST_RATIO', 'min'),\n",
    "   ('CNT_INST_RATIO', 'max'),\n",
    "   ('CNT_INST_RATIO', 'median'),\n",
    "   ('REM_INST_RATIO', 'mean'),\n",
    "   ('REM_INST_RATIO', 'min'),\n",
    "   ('REM_INST_RATIO', 'max'),\n",
    "   ('REM_INST_RATIO', 'median'),\n",
    "   ('PAID_INST_RATIO', 'mean'),\n",
    "   ('PAID_INST_RATIO', 'min'),\n",
    "   ('PAID_INST_RATIO', 'max'),\n",
    "   ('PAID_INST_RATIO', 'median'),\n",
    "   ('INST_OF_FIRST_DPD', 'mean'),\n",
    "   ('INST_OF_FIRST_DPD', 'min'),\n",
    "   ('INST_OF_FIRST_DPD', 'max'),\n",
    "   ('INST_OF_FIRST_DPD', 'median'),\n",
    "   ('FIRST_DPD_PERCENTILE', 'mean'),\n",
    "   ('FIRST_DPD_PERCENTILE', 'min'),\n",
    "   ('FIRST_DPD_PERCENTILE', 'max'),\n",
    "   ('FIRST_DPD_PERCENTILE', 'median'),\n",
    "   ('INST_OF_FIRST_DPD_DEF', 'mean'),\n",
    "   ('INST_OF_FIRST_DPD_DEF', 'min'),\n",
    "   ('INST_OF_FIRST_DPD_DEF', 'max'),\n",
    "   ('INST_OF_FIRST_DPD_DEF', 'median'),\n",
    "   ('FIRST_DPD_DEF_PERCENTILE', 'mean'),\n",
    "   ('FIRST_DPD_DEF_PERCENTILE', 'min'),\n",
    "   ('FIRST_DPD_DEF_PERCENTILE', 'max'),\n",
    "   ('FIRST_DPD_DEF_PERCENTILE', 'median')])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_features = pd.DataFrame({'SK_ID_CURR': features['SK_ID_CURR'].unique()})\n",
    "for groupby_cols, specs in POS_CASH_BALANCE_AGGREGATION_RECIPIES:\n",
    "    group_object = features.groupby(groupby_cols)\n",
    "    for select, agg in specs:\n",
    "        groupby_aggregate_name = '{}_{}_{}'.format('_'.join(groupby_cols), agg, select)\n",
    "        agg_features = agg_features.merge(group_object[select]\n",
    "                              .agg(agg)\n",
    "                              .reset_index()\n",
    "                              .rename(columns={select: groupby_aggregate_name})\n",
    "                              [groupby_cols + [groupby_aggregate_name]],\n",
    "                              on=groupby_cols,\n",
    "                              how='left')  \n",
    "\n",
    "agg_features.rename(columns=lambda x: x.replace('SK_ID_CURR_', 'AGG_'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_pos_cash_balance = agg_pos_cash_balance.merge(agg_features, on=['SK_ID_CURR'], how='left')\n",
    "del agg_features, features\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_cash_sorted = pos_cash_balance.sort_values(['SK_ID_CURR','SK_ID_PREV','MONTHS_BALANCE'])\n",
    "tmp_pos_bal = pos_cash_sorted.groupby(['SK_ID_CURR','SK_ID_PREV'])['NAME_CONTRACT_STATUS'].last().reset_index()\n",
    "tmp_pos_bal.rename(columns={'NAME_CONTRACT_STATUS':'LAST_STATUS'}, inplace=True)\n",
    "tmp_pos_bal, cat_cols = one_hot_encoder(tmp_pos_bal, nan_as_category= False)\n",
    "\n",
    "cat_cols.extend(('SK_ID_CURR','SK_ID_PREV'))\n",
    "Label=[s+'_'+l for s in cat_cols if s not in ['SK_ID_PREV','SK_ID_CURR'] for l in ['mean']]\n",
    "agg_pos_bal_cat = tmp_pos_bal[cat_cols].groupby(['SK_ID_PREV','SK_ID_CURR'])\\\n",
    "            .agg(['mean']).groupby(level='SK_ID_CURR')\\\n",
    "            .agg('mean').round(2).reset_index()\n",
    "agg_pos_bal_cat.columns=['SK_ID_CURR']+Label\n",
    "agg_pos_bal_cat.drop(['LAST_STATUS_Amortized debt_mean','LAST_STATUS_Approved_mean','LAST_STATUS_Canceled_mean','LAST_STATUS_Demand_mean','LAST_STATUS_Returned to the store_mean','LAST_STATUS_Signed_mean'], axis=1, inplace=True)\n",
    "\n",
    "## Merge Numerical and Cat features\n",
    "agg_pos_cash_balance = agg_pos_cash_balance.merge(agg_pos_bal_cat, on='SK_ID_CURR')\n",
    "\n",
    "del tmp_pos_bal, agg_pos_bal_cat, pos_cash_sorted\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_cash_balance['POS_CASH_PAID_LATE'] = (pos_cash_balance['SK_DPD'] > 0).astype(int)\n",
    "pos_cash_balance['POS_CASH_PAID_LATE_WITH_TOLERANCE'] = (pos_cash_balance['SK_DPD_DEF'] > 0).astype(int)\n",
    "groupby = pos_cash_balance.groupby(['SK_ID_CURR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 34/34.0 [27:43<00:00, 48.94s/it]\n"
     ]
    }
   ],
   "source": [
    "func = partial(last_k_installment_features, periods=[1, 10, 50, 10e16])\n",
    "g = parallel_apply(groupby, func, index_name='SK_ID_CURR', num_workers=10, chunk_size=10000).reset_index()\n",
    "last_k_installment = g.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(337252, 53)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_k_installment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(337252, 109)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_pos_cash_balance = agg_pos_cash_balance.merge(last_k_installment, on='SK_ID_CURR',how='inner')\n",
    "agg_pos_cash_balance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 34/34.0 [13:19<00:00, 23.53s/it]\n"
     ]
    }
   ],
   "source": [
    "g = parallel_apply(groupby, last_loan_features, index_name='SK_ID_CURR', num_workers=10, chunk_size=10000).reset_index()\n",
    "last_loan = g.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(337252, 16)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_loan.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(337252, 124)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_pos_cash_balance = agg_pos_cash_balance.merge(last_loan, on='SK_ID_CURR',how='inner')\n",
    "agg_pos_cash_balance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "func = partial(trend_in_last_k_installment_features, periods=[3,6,12,30,60])\n",
    "g = parallel_apply(groupby, func, index_name='SK_ID_CURR', num_workers=10, chunk_size=10000).reset_index()\n",
    "trend_in_last_k_installment = g.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(337252, 134)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_pos_cash_balance = agg_pos_cash_balance.merge(trend_in_last_k_installment, on='SK_ID_CURR',how='inner')\n",
    "agg_pos_cash_balance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping columns that are not useful\n",
    "ColToDrop = ['ALL_INSTALLMENT_POS_CASH_PAID_LATE_count','ALL_INSTALLMENT_POS_CASH_PAID_LATE_mean',\n",
    "             'ALL_INSTALLMENT_SK_DPD_DEF_mean','ALL_INSTALLMENT_SK_DPD_DEF_max','ALL_INSTALLMENT_SK_DPD_DEF_median',\n",
    "             'ALL_INSTALLMENT_SK_DPD_DEF_min','ALL_INSTALLMENT_SK_DPD_mean','ALL_INSTALLMENT_SK_DPD_sum',\n",
    "             'ALL_INSTALLMENT_SK_DPD_min','ALL_INSTALLMENT_SK_DPD_median','ALL_INSTALLMENT_SK_DPD_max',\n",
    "             'LAST_LOAN_SK_DPD_DEF_min','LAST_LOAN_SK_DPD_min',\n",
    "             'LAST_1_SK_DPD_max','LAST_1_SK_DPD_min','LAST_1_SK_DPD_mean','LAST_1_SK_DPD_median',\n",
    "             'LAST_1_SK_DPD_DEF_max','LAST_1_SK_DPD_DEF_min','LAST_1_SK_DPD_DEF_mean','LAST_1_SK_DPD_DEF_median',\n",
    "             'LAST_1_POS_CASH_PAID_LATE_count','LAST_1_POS_CASH_PAID_LATE_mean',\n",
    "             'LAST_10_SK_DPD_DEF_min','LAST_10_SK_DPD_min', 'LAST_50_SK_DPD_DEF_median','LAST_50_SK_DPD_DEF_min',\n",
    "             'LAST_50_SK_DPD_max','LAST_50_SK_DPD_median','LAST_50_SK_DPD_min',\n",
    "             '3_PERIOD_TREND_SK_DPD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(337252, 103)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_pos_cash_balance.drop(ColToDrop, axis=1, inplace=True)\n",
    "del pos_cash_balance\n",
    "gc.collect()\n",
    "agg_pos_cash_balance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute missing values\n",
    "## Fill std columns with zero as there is only 1 month data for Nan values\n",
    "std_cols = [col for col in agg_pos_cash_balance.columns if '_std' in col]\n",
    "agg_pos_cash_balance.update(agg_pos_cash_balance[std_cols].fillna(0))\n",
    "del std_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(337252, 103)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Label=['POS_' + e for e in agg_pos_cash_balance.columns.tolist() if 'SK_ID_CURR' not in e]\n",
    "agg_pos_cash_balance.columns = ['SK_ID_CURR']+Label\n",
    "agg_pos_cash_balance.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe: 250.23 MB\n",
      "Memory usage after optimization: 87.16 MB\n",
      "Decreased by 65.2%\n"
     ]
    }
   ],
   "source": [
    "agg_pos_cash_balance = reduce_mem_usage(agg_pos_cash_balance, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "\n",
      "Memory usage of dataframe: 222.62 MB\n",
      "Memory usage after optimization: 112.95 MB\n",
      "Decreased by 49.3%\n",
      "Memory usage of dataframe: 624.85 MB\n",
      "Memory usage after optimization: 338.46 MB\n",
      "Decreased by 45.8%\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\\n\")\n",
    "bureau = pd.read_csv(dir+'bureau.csv')\n",
    "bb = pd.read_csv(dir+'bureau_balance.csv')\n",
    "\n",
    "bureau = reduce_mem_usage(bureau, verbose=True)\n",
    "bb = reduce_mem_usage(bb, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess bureau.csv and bureau_balance.csv\n",
    "bb_agg = pd.DataFrame({'SK_ID_BUREAU': bb['SK_ID_BUREAU'].unique()})\n",
    "\n",
    "# Some new features in bureau_balance set\n",
    "bb_aggregations = {'MONTHS_BALANCE': ['size']}\n",
    "tmp = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations).reset_index()\n",
    "tmp.columns = ['SK_ID_BUREAU','Month']\n",
    "bb_agg = bb_agg.merge(tmp, on=['SK_ID_BUREAU'], how='left')\n",
    "del bb_aggregations, tmp\n",
    "\n",
    "tmp = bb[['SK_ID_BUREAU', 'STATUS']].groupby('SK_ID_BUREAU')\n",
    "tmp_last = tmp.last().reset_index()\n",
    "tmp_last.rename(columns={'STATUS':'First_status'}, inplace=True)\n",
    "bb_agg = bb_agg.merge(tmp_last, on=['SK_ID_BUREAU'], how='left')\n",
    "tmp_first = tmp.first().reset_index()\n",
    "tmp_first.rename(columns={'STATUS':'Last_status'}, inplace=True)\n",
    "bb_agg = bb_agg.merge(tmp_first, on=['SK_ID_BUREAU'], how='left')\n",
    "del tmp, tmp_first, tmp_last\n",
    "gc.collect()\n",
    "\n",
    "tmp = bb.loc[bb['STATUS'] == 'C', ['SK_ID_BUREAU', 'MONTHS_BALANCE']] \\\n",
    "                .groupby('SK_ID_BUREAU').last().reset_index()\n",
    "tmp = tmp.apply(abs)\n",
    "tmp.rename(columns={'MONTHS_BALANCE':'When_closed'}, inplace=True)\n",
    "bb_agg = bb_agg.merge(tmp, on=['SK_ID_BUREAU'], how='left')\n",
    "del tmp\n",
    "bb_agg['Month_closed_to_end'] = bb_agg['Month'] - bb_agg['When_closed']\n",
    "\n",
    "bb_agg.update(bb_agg['When_closed'].fillna(-1))\n",
    "bb_agg.update(bb_agg['Month_closed_to_end'].fillna(-1))\n",
    "gc.collect()\n",
    "\n",
    "tmp = bb.loc[bb['STATUS'] == 'X', ['SK_ID_BUREAU', 'MONTHS_BALANCE']] \\\n",
    "                         .groupby('SK_ID_BUREAU').count().reset_index()\n",
    "tmp.rename(columns={'MONTHS_BALANCE':'X_cnt'}, inplace=True)\n",
    "bb_agg = bb_agg.merge(tmp, on=['SK_ID_BUREAU'], how='left')\n",
    "bb_agg['X_per_Month'] = bb_agg['X_cnt'] / bb_agg['Month']\n",
    "\n",
    "for c in range(6):\n",
    "    tmp = bb.loc[bb['STATUS'] == str(c), ['SK_ID_BUREAU', 'MONTHS_BALANCE']] \\\n",
    "                         .groupby('SK_ID_BUREAU').count()\n",
    "    tmp.columns = ['DPD_' + str(c) + '_cnt']\n",
    "    bb_agg = bb_agg.join(tmp, on=['SK_ID_BUREAU'], how='left')\n",
    "    bb_agg['DPD_' + str(c) + '_per_Month'] = bb_agg['DPD_' + str(c) + '_cnt'] / bb_agg['Month']\n",
    "    del tmp\n",
    "    gc.collect()\n",
    "\n",
    "NA_col = ['X_cnt','X_per_Month','DPD_0_cnt','DPD_0_per_Month','DPD_1_cnt','DPD_1_per_Month','DPD_2_cnt',\n",
    "          'DPD_2_per_Month','DPD_3_cnt','DPD_3_per_Month','DPD_4_cnt','DPD_4_per_Month','DPD_5_cnt','DPD_5_per_Month']\n",
    "bb_agg.update(bb_agg[NA_col].fillna(0))\n",
    "bb_agg['Non_zero_DPD_cnt'] = bb_agg[['DPD_1_cnt', 'DPD_2_cnt', 'DPD_3_cnt', 'DPD_4_cnt', 'DPD_5_cnt']].sum(axis = 1)\n",
    "del NA_col\n",
    "gc.collect()\n",
    "\n",
    "bb_agg, bb_cat = one_hot_encoder(bb_agg, nan_as_category = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing..1 - Order is important\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"preprocessing..1 - Order is important\\n\")\n",
    "\n",
    "bureau['DAYS_CREDIT_ENDDATE'] = bureau['DAYS_CREDIT_ENDDATE'].combine_first(bureau['DAYS_ENDDATE_FACT'])\n",
    "bureau.update(bureau['DAYS_CREDIT_ENDDATE'].fillna(9999))\n",
    "\n",
    "d = {\"Closed\": 99991,\n",
    "    \"Sold\": 9999, \"Bad debt\": 9999}\n",
    "s = bureau.CREDIT_ACTIVE.map(d)\n",
    "bureau['DAYS_ENDDATE_FACT'] = bureau.DAYS_ENDDATE_FACT.combine_first(s)\n",
    "bureau['DAYS_ENDDATE_FACT'] = np.where(bureau['DAYS_ENDDATE_FACT'] == 99991, bureau['DAYS_CREDIT_UPDATE'], bureau['DAYS_ENDDATE_FACT'])\n",
    "del d,s\n",
    "gc.collect()\n",
    "\n",
    "#Fill missing values\n",
    "# bureau.update(bureau[['AMT_CREDIT_SUM_DEBT','AMT_CREDIT_SUM','AMT_CREDIT_MAX_OVERDUE','AMT_CREDIT_SUM_LIMIT','AMT_ANNUITY']].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing..2\n",
      "\n",
      "Mean imputation for buro_bal started..\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"preprocessing..2\\n\")\n",
    "\n",
    "di = {\"Another type of loan\" : \"Other\",\"Car loan\": \"Personal\",\"Cash loan (non-earmarked)\": \"Personal\",\n",
    "      \"Consumer credit\": \"Personal\",\"Credit card\": \"Personal\",\"Interbank credit\": \"Other\",\"Loan for business development\": \"Business\",\n",
    "      \"Loan for purchase of shares (margin lending)\": \"Business\", \"Loan for the purchase of equipment\": \"Business\",\n",
    "      \"Loan for working capital replenishment\" : \"Business\",\"Microloan\": \"Business\",\"Mobile operator loan\" : \"Personal\", \n",
    "      \"Unknown type of loan\" : \"Other\", \"Mortgage\": \"Other\", \"Real estate loan\" : \"Real_Estate\"}\n",
    "bureau['CREDIT_TYPE'] = bureau['CREDIT_TYPE'].map(di)\n",
    "\n",
    "bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category = False)\n",
    "\n",
    "#New features\n",
    "grp = bureau[['SK_ID_CURR', 'SK_ID_BUREAU', 'DAYS_CREDIT']].groupby(by = ['SK_ID_CURR'])\n",
    "grp1 = grp.apply(lambda x: x.sort_values(['DAYS_CREDIT'], ascending = False)).reset_index(drop = True)\n",
    "bureau['DAYS_DIFF'] = grp1.groupby(by = ['SK_ID_CURR'])['DAYS_CREDIT'].diff().abs()\n",
    "bureau['INIT_LOAN_PERIOD'] = bureau['DAYS_CREDIT_ENDDATE'] - bureau['DAYS_CREDIT']\n",
    "bureau['ACT_LOAN_PERIOD'] = bureau['DAYS_ENDDATE_FACT'] - bureau['DAYS_CREDIT']\n",
    " \n",
    "# Bureau balance: Merge with bureau.csv\n",
    "bureau = bureau.merge(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n",
    "\n",
    "print(\"Mean imputation for buro_bal started..\\n\")\n",
    "\n",
    "bb_agg_col = [f for f in bb_agg.columns if f != 'SK_ID_BUREAU' if f not in bb_cat]\n",
    "\n",
    "# #Fill missing bureau_balance data with mean\n",
    "# for c in bb_agg_col:\n",
    "#     bureau[c] = bureau.groupby(by='SK_ID_CURR')[c].transform(lambda x: x.fillna(x.mean()))\n",
    "#     bureau.update(bureau[c].fillna(0))\n",
    "#     print(c+\" imputation completed\\n\")\n",
    "#     gc.collect()\n",
    "\n",
    "del bb, bb_agg, bb_agg_col\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bureau and bureau_balance numeric features\n",
    "num_aggregations = {\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_ENDDATE': ['mean','max','min','median'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean','min'],\n",
    "        'DAYS_ENDDATE_FACT': ['mean','max','min'],\n",
    "        'CREDIT_DAY_OVERDUE': ['max','median','sum'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean','max'],\n",
    "        'AMT_CREDIT_SUM': ['max','min', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean'],\n",
    "        'AMT_ANNUITY': ['max','sum'],\n",
    "        'DAYS_DIFF' : ['mean','max','min','median','var'],\n",
    "        'INIT_LOAN_PERIOD' : ['mean','max','min','median','var','sum'],\n",
    "        'ACT_LOAN_PERIOD' : ['mean','max','min','median','var','sum'],\n",
    "        'Month' :['max','min', 'mean'],\n",
    "        'When_closed':['max','mean', 'sum'],\n",
    "        'Month_closed_to_end':['max','min', 'mean'],\n",
    "        'X_cnt':['max','min', 'mean', 'sum'],\n",
    "        'X_per_Month':['max','min', 'mean', 'sum'],\n",
    "        'DPD_0_cnt':['mean', 'sum'],\n",
    "        'DPD_0_per_Month':['mean'],\n",
    "        'DPD_1_cnt':['mean', 'sum'],\n",
    "        'DPD_1_per_Month':['mean'],\n",
    "        'DPD_2_cnt':['mean', 'sum'],\n",
    "        'DPD_2_per_Month':['mean'],\n",
    "        'DPD_3_cnt':['mean', 'sum'],\n",
    "        'DPD_3_per_Month':['mean'],\n",
    "        'DPD_4_cnt':['mean', 'sum'],\n",
    "        'DPD_4_per_Month':['mean'],\n",
    "        'DPD_5_cnt':['mean', 'sum'],\n",
    "        'DPD_5_per_Month':['mean'],\n",
    "        'Non_zero_DPD_cnt':['mean', 'sum']\n",
    "    }\n",
    "    \n",
    "# Bureau and bureau_balance categorical features\n",
    "\n",
    "cat_aggregations = {}\n",
    "for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n",
    "for cat in bb_cat: cat_aggregations[cat] = ['mean']\n",
    "bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations}).reset_index()\n",
    "bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "bureau_agg.rename(columns={'BURO_SK_ID_CURR_':'SK_ID_CURR'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_agg['BURO_DEBT_CREDIT_RATIO'] = bureau_agg['BURO_AMT_CREDIT_SUM_DEBT_SUM']/(1 + bureau_agg['BURO_AMT_CREDIT_SUM_SUM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bureau: Active credits - using only numerical aggregations\n",
    "active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations).reset_index()\n",
    "active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "active_agg.rename(columns={'ACTIVE_SK_ID_CURR_':'SK_ID_CURR'}, inplace=True)\n",
    "bureau_agg = bureau_agg.merge(active_agg, how='left', on='SK_ID_CURR')\n",
    "del active, active_agg\n",
    "gc.collect()\n",
    "# Bureau: Closed credits - using only numerical aggregations\n",
    "closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations).reset_index()\n",
    "closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "closed_agg.rename(columns={'CLOSED_SK_ID_CURR_':'SK_ID_CURR'}, inplace=True)\n",
    "bureau_agg = bureau_agg.merge(closed_agg, how='left', on='SK_ID_CURR')\n",
    "del closed, closed_agg, bureau\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureau_agg['BURO_BB_MISSING'] = bureau_agg['BURO_Month_MEAN'].isnull().astype(int)\n",
    "bureau_agg['ACTIVE_BURO_BB_MISSING'] = bureau_agg['ACTIVE_Month_MEAN'].isnull().astype(int)\n",
    "bureau_agg['CLOSED_BURO_BB_MISSING'] = bureau_agg['CLOSED_Month_MEAN'].isnull().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute missing values\n",
    "## Fill columns from BB which has no data in BURO table\n",
    "buro_bb_cols = [col for col in bureau_agg.columns if any(['Month' in col,'When_closed' in col, '_X_' in col, '_DPD_' in col, 'First_' in col,'Last_' in col]) if 'BURO' in col]\n",
    "bureau_agg.update(bureau_agg[buro_bb_cols].fillna(bureau_agg[buro_bb_cols].mean()))\n",
    "del buro_bb_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute missing values\n",
    "## Fill days columns with a value to distinguish\n",
    "days_cols = [col for col in bureau_agg.columns if any(['ACTIVE' in col,'CLOSED' in col]) if 'DAY' in col if '_DIFF' not in col]\n",
    "bureau_agg.update(bureau_agg[days_cols].fillna(999))\n",
    "days_diff_cols = [col for col in bureau_agg.columns if any(['ACTIVE' in col,'CLOSED' in col]) if '_DIFF' in col]\n",
    "bureau_agg.update(bureau_agg[days_diff_cols].fillna(-1))\n",
    "\n",
    "del days_cols, days_diff_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute missing values\n",
    "## ACTIVE / CLOSED can have missing values for 2 reasons - missing BB data or missing ACTIVE/CLOSED credits\n",
    "active_bb_cols = [col for col in bureau_agg.columns if any(['Month' in col,'When_closed' in col, '_X_' in col, '_DPD_' in col, 'First_' in col,'Last_' in col]) if 'ACTIVE' in col]\n",
    "closed_bb_cols = [col for col in bureau_agg.columns if any(['Month' in col,'When_closed' in col, '_X_' in col, '_DPD_' in col, 'First_' in col,'Last_' in col]) if 'CLOSED' in col]\n",
    "active_cols = [col for col in bureau_agg.columns if 'ACTIVE' in col if 'BURO' not in col]\n",
    "closed_cols = [col for col in bureau_agg.columns if 'CLOSED' in col if 'BURO' not in col]\n",
    "\n",
    "bureau_agg.loc[bureau_agg['BURO_BB_MISSING'] == 1, active_bb_cols] = bureau_agg.fillna(bureau_agg[active_bb_cols].mean())\n",
    "bureau_agg.loc[bureau_agg['BURO_BB_MISSING'] == 0, active_cols] = bureau_agg.fillna(-99)\n",
    "\n",
    "bureau_agg.loc[bureau_agg['BURO_BB_MISSING'] == 1, closed_bb_cols] = bureau_agg.fillna(bureau_agg[closed_bb_cols].mean())\n",
    "bureau_agg.loc[bureau_agg['BURO_BB_MISSING'] == 0, closed_cols] = bureau_agg.fillna(-99)\n",
    "\n",
    "del active_bb_cols, closed_bb_cols, active_cols, closed_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute missing values\n",
    "## Fill vars columns with zero as there is only 1 month data for Nan values\n",
    "var_cols = [col for col in bureau_agg.columns if '_VAR' in col]\n",
    "bureau_agg.update(bureau_agg[var_cols].fillna(0))\n",
    "del var_cols, bb_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(305811, 303)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bureau_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe: 615.45 MB\n",
      "Memory usage after optimization: 223.78 MB\n",
      "Decreased by 63.6%\n"
     ]
    }
   ],
   "source": [
    "bureau_agg = reduce_mem_usage(bureau_agg, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading data...\")\n",
    "ins = pd.read_csv(dir+'installments_payments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess installments_payments.csv\n",
    "### Fill missing values by values to distinguish\n",
    "# ins.update(ins['DAYS_ENTRY_PAYMENT'].fillna(999))\n",
    "ins.update(ins['AMT_PAYMENT'].fillna(0))\n",
    "# Percentage and difference paid in each installment (amount paid and installment value)\n",
    "ins['PAYMENT_RATIO'] = ins['AMT_PAYMENT'] / (1 + ins['AMT_INSTALMENT'])\n",
    "ins['PAYMENT_DIFF'] = ins['AMT_PAYMENT'] - ins['AMT_INSTALMENT'] \n",
    "# Days past due and days before due (no negative values)\n",
    "ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "ins['INST_PAID_LATE'] = (ins['DPD'] > 0).astype(int)\n",
    "ins['INST_PAID_OVER'] = (ins['PAYMENT_DIFF'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ins_sorted = ins.sort_values(['SK_ID_CURR','SK_ID_PREV','DAYS_INSTALMENT'])\n",
    "\n",
    "ins_sorted['NEW_INST_NUM']=ins_sorted.groupby(['SK_ID_CURR','SK_ID_PREV']).cumcount() + 1\n",
    "ins_sorted.drop(['NUM_INSTALMENT_VERSION','NUM_INSTALMENT_NUMBER'], axis=1, inplace=True)\n",
    "\n",
    "ins_sorted['DPD_CUMSUM'] = ins_sorted.groupby(['SK_ID_CURR','SK_ID_PREV'])['DPD'].cumsum()\n",
    "\n",
    "features = ins_sorted[['SK_ID_CURR','SK_ID_PREV']].drop_duplicates()\n",
    "group_object1 = ins_sorted.groupby(['SK_ID_CURR','SK_ID_PREV'])['NEW_INST_NUM'].count().reset_index()\n",
    "group_object1.rename(columns={'NEW_INST_NUM': 'TOT_INST'},inplace=True)\n",
    "\n",
    "ins_sorted_DPD = ins_sorted.loc[ins_sorted.DPD_CUMSUM > 0]\n",
    "group_object2 = ins_sorted_DPD.groupby(['SK_ID_CURR','SK_ID_PREV'])['NEW_INST_NUM'].first().reset_index()\n",
    "group_object2.rename(columns={'NEW_INST_NUM': 'INST_OF_FIRST_DPD'},inplace=True)\n",
    "\n",
    "ins_sorted_DPD_60 = ins_sorted.loc[ins_sorted.DPD_CUMSUM >= 60]\n",
    "group_object3 = ins_sorted_DPD_60.groupby(['SK_ID_CURR','SK_ID_PREV'])['NEW_INST_NUM'].first().reset_index()\n",
    "group_object3.rename(columns={'NEW_INST_NUM': 'INST_OF_60_DPD'},inplace=True)\n",
    "\n",
    "features = features.merge(group_object1, on=['SK_ID_CURR','SK_ID_PREV'], how='left')\\\n",
    "                   .merge(group_object2, on=['SK_ID_CURR','SK_ID_PREV'], how='left')\\\n",
    "                   .merge(group_object3, on=['SK_ID_CURR','SK_ID_PREV'], how='left').reset_index(drop=True)\n",
    "        \n",
    "features['FIRST_DPD_PERCENTILE'] =  features['INST_OF_FIRST_DPD']/features['TOT_INST']\n",
    "features['60_DPD_PERCENTILE'] =  features['INST_OF_60_DPD']/features['TOT_INST']\n",
    "\n",
    "del ins_sorted, group_object1, group_object2, group_object3, ins_sorted_DPD, ins_sorted_DPD_60\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "INS_AGGREGATION_RECIPIES  = [(['SK_ID_CURR'],\n",
    "  [('TOT_INST', 'mean'),\n",
    "   ('TOT_INST', 'sum'),\n",
    "   ('TOT_INST', 'max'),\n",
    "   ('TOT_INST', 'median'),\n",
    "   ('INST_OF_FIRST_DPD', 'mean'),\n",
    "   ('INST_OF_FIRST_DPD', 'min'),\n",
    "   ('INST_OF_FIRST_DPD', 'max'),\n",
    "   ('INST_OF_FIRST_DPD', 'median'),\n",
    "   ('INST_OF_60_DPD', 'mean'),\n",
    "   ('INST_OF_60_DPD', 'min'),\n",
    "   ('INST_OF_60_DPD', 'max'),\n",
    "   ('INST_OF_60_DPD', 'median'),\n",
    "   ('FIRST_DPD_PERCENTILE', 'mean'),\n",
    "   ('FIRST_DPD_PERCENTILE', 'min'),\n",
    "   ('FIRST_DPD_PERCENTILE', 'max'),\n",
    "   ('FIRST_DPD_PERCENTILE', 'median'),\n",
    "   ('60_DPD_PERCENTILE', 'mean'),\n",
    "   ('60_DPD_PERCENTILE', 'min'),\n",
    "   ('60_DPD_PERCENTILE', 'max'),\n",
    "   ('60_DPD_PERCENTILE', 'median')])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_features = pd.DataFrame({'SK_ID_CURR': features['SK_ID_CURR'].unique()})\n",
    "for groupby_cols, specs in INS_AGGREGATION_RECIPIES:\n",
    "    group_object = features.groupby(groupby_cols)\n",
    "    for select, agg in specs:\n",
    "        groupby_aggregate_name = '{}_{}_{}'.format('_'.join(groupby_cols), agg, select)\n",
    "        agg_features = agg_features.merge(group_object[select]\n",
    "                              .agg(agg)\n",
    "                              .reset_index()\n",
    "                              .rename(columns={select: groupby_aggregate_name})\n",
    "                              [groupby_cols + [groupby_aggregate_name]],\n",
    "                              on=groupby_cols,\n",
    "                              how='left')  \n",
    "\n",
    "agg_features.rename(columns=lambda x: x.replace('SK_ID_CURR_', 'AGG_'), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(339587, 56)\n"
     ]
    }
   ],
   "source": [
    "# Features: Perform aggregations\n",
    "aggregations = {\n",
    "            'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max', 'mean', 'sum', 'var', 'median'],\n",
    "        'DBD': ['max', 'mean', 'sum'],\n",
    "        'PAYMENT_RATIO': ['max', 'mean', 'var','min','median'],\n",
    "        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_INSTALMENT': ['max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['min','max', 'mean', 'sum'],\n",
    "        'INST_PAID_LATE': ['sum','mean']\n",
    "    }\n",
    "\n",
    "ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "# Count installments accounts\n",
    "ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "ins_agg = ins_agg.reset_index()\n",
    "\n",
    "ins_agg = ins_agg.merge(agg_features, on=['SK_ID_CURR'], how='left')\n",
    "del agg_features, features\n",
    "gc.collect()\n",
    "\n",
    "print (ins_agg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins['INST_PAID_LATE_IN_DAYS'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT'] \n",
    "ins['INST_PAID_LATE'] = (ins['INST_PAID_LATE_IN_DAYS'] > 0).astype(int)\n",
    "ins['INST_PAID_OVER_AMT'] = ins['AMT_PAYMENT'] - ins['AMT_INSTALMENT']\n",
    "ins['INST_PAID_OVER'] = (ins['INST_PAID_OVER_AMT'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupby = ins.groupby(['SK_ID_CURR'])\n",
    "opensol_ins_agg = pd.DataFrame({'SK_ID_CURR': ins['SK_ID_CURR'].unique()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "func = partial(trend_in_last_k_instalment_features, periods=[10,50,100,500])\n",
    "\n",
    "g = parallel_apply(groupby, func, index_name='SK_ID_CURR',\n",
    "                   num_workers=16, chunk_size=10000).reset_index()\n",
    "\n",
    "trend_in_last_k_ins = g.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(339587, 9)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trend_in_last_k_ins.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(339587, 9)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opensol_ins_agg = opensol_ins_agg.merge(trend_in_last_k_ins, on='SK_ID_CURR', how='inner')\n",
    "opensol_ins_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "func = partial(last_k_instalment_features_with_fractions, \n",
    "               periods=[1,5,10,20,50,100],\n",
    "               fraction_periods=[(5,20),(5,50),(10,100)])\n",
    "\n",
    "g = parallel_apply(groupby, func, index_name='SK_ID_CURR',\n",
    "                   num_workers=16, chunk_size=1000).reset_index()\n",
    "\n",
    "last_k_ins_with_fractions = g.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(339587, 190)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_k_ins_with_fractions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(339587, 198)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opensol_ins_agg = opensol_ins_agg.merge(last_k_ins_with_fractions, on='SK_ID_CURR', how='inner')\n",
    "opensol_ins_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    " ins_redundant_cols = [\n",
    " 'LAST_1_INST_PAID_LATE_IN_DAYS_mean',\n",
    " 'LAST_1_INST_PAID_LATE_IN_DAYS_median',\n",
    " 'LAST_1_INST_PAID_LATE_IN_DAYS_min',\n",
    " 'LAST_1_INST_PAID_LATE_IN_DAYS_sum',\n",
    " 'LAST_1_INST_PAID_OVER_AMT_mean',\n",
    " 'LAST_1_INST_PAID_OVER_AMT_median',\n",
    " 'LAST_1_INST_PAID_OVER_AMT_min',\n",
    " 'LAST_1_INST_PAID_OVER_AMT_sum',\n",
    " 'LAST_1_NUM_INSTALMENT_VERSION_mean',\n",
    " 'LAST_1_NUM_INSTALMENT_VERSION_median',\n",
    " 'LAST_1_NUM_INSTALMENT_VERSION_min',\n",
    " 'LAST_1_NUM_INSTALMENT_VERSION_sum',\n",
    " '10_PERIOD_TREND_INST_PAID_OVER_AMT'\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(339587, 185)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opensol_ins_agg.drop(ins_redundant_cols, axis = 1, inplace=True)\n",
    "del ins\n",
    "opensol_ins_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(339587, 240)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ins_agg = ins_agg.merge(opensol_ins_agg, on='SK_ID_CURR', how='inner')\n",
    "ins_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237\n"
     ]
    }
   ],
   "source": [
    "ins_agg_col = non_missing_col(ins_agg)\n",
    "print(len(ins_agg_col))\n",
    "ins_agg = ins_agg[ins_agg_col]\n",
    "del ins_agg_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute missing values\n",
    "## Fill vars columns with zero as there is only 1 month data for Nan values\n",
    "var_cols = [col for col in ins_agg.columns if '_VAR' in col]\n",
    "ins_agg.update(ins_agg[var_cols].fillna(0))\n",
    "del var_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fill std columns with zero as there is only 1 month data for Nan values\n",
    "std_cols = [col for col in ins_agg.columns if '_std' in col]\n",
    "ins_agg[std_cols] = ins_agg[std_cols].replace([np.inf, -np.inf], np.nan)\n",
    "ins_agg.update(ins_agg[std_cols].fillna(0))\n",
    "del std_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe: 616.62 MB\n",
      "Memory usage after optimization: 199.82 MB\n",
      "Decreased by 67.6%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(339587, 237)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ins_agg = reduce_mem_usage(ins_agg, verbose=True)\n",
    "ins_agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %store ins_agg\n",
    "# %store agg_pos_cash_balance\n",
    "# %store bureau_agg\n",
    "# %store prev_agg\n",
    "# %store agg_cred_card_bal\n",
    "# %store -z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe: 286.23 MB\n",
      "Memory usage after optimization: 92.38 MB\n",
      "Decreased by 67.7%\n",
      "Memory usage of dataframe: 45.00 MB\n",
      "Memory usage after optimization: 14.60 MB\n",
      "Decreased by 67.6%\n"
     ]
    }
   ],
   "source": [
    "tr = pd.read_csv(dir+'application_train.csv') \n",
    "tr = reduce_mem_usage(tr, verbose=True)\n",
    "\n",
    "te = pd.read_csv(dir+'application_test.csv')\n",
    "te = reduce_mem_usage(te, verbose=True)\n",
    "\n",
    "tri=tr.shape[0]\n",
    "y = tr.TARGET.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(193910, 2) (668, 2) (69633, 2)\n",
      "(193910, 1) (668, 1) (69633, 1)\n",
      "(193910, 2) (668, 2) (69633, 2)\n",
      "(356255, 122)\n",
      "(307511, 122) (48744, 122)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "523"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #Ext_Source_Imputation - Failed attempt\n",
    "# tr_EXT = tr[['SK_ID_CURR','EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3']]\n",
    "# te_EXT = te[['SK_ID_CURR','EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3']]\n",
    "# tr_te_EXT = tr_EXT.append(te_EXT, sort=False)\n",
    "# EXT_1_null = tr_te_EXT[['SK_ID_CURR','EXT_SOURCE_1']].loc[tr_te_EXT.EXT_SOURCE_1.isnull()].reset_index(drop=True)\n",
    "# EXT_2_null = tr_te_EXT[['SK_ID_CURR','EXT_SOURCE_2']].loc[tr_te_EXT.EXT_SOURCE_2.isnull()].reset_index(drop=True)\n",
    "# EXT_3_null = tr_te_EXT[['SK_ID_CURR','EXT_SOURCE_3']].loc[tr_te_EXT.EXT_SOURCE_3.isnull()].reset_index(drop=True)\n",
    "# print(EXT_1_null.shape, EXT_2_null.shape, EXT_3_null.shape)\n",
    "# EXT_1 = pd.read_csv(\"Ext_Source/EXT_SOURCE_1.csv\") \n",
    "# EXT_2 = pd.read_csv(\"Ext_Source/EXT_SOURCE_2.csv\") \n",
    "# EXT_3 = pd.read_csv(\"Ext_Source/EXT_SOURCE_3.csv\") \n",
    "# print(EXT_1.shape, EXT_2.shape, EXT_3.shape)\n",
    "# EXT_1 = EXT_1_null.combine_first(EXT_1).drop('EXT_SOURCE_1', axis=1)\n",
    "# EXT_2 = EXT_2_null.combine_first(EXT_2).drop('EXT_SOURCE_2', axis=1)\n",
    "# EXT_3 = EXT_3_null.combine_first(EXT_3).drop('EXT_SOURCE_3', axis=1)\n",
    "# print(EXT_1.shape, EXT_2.shape, EXT_3.shape)\n",
    "# tr_te = tr.append(te, sort=False)\n",
    "# tr_te.shape\n",
    "# tr_te = (tr_te.merge(EXT_1,on='SK_ID_CURR',how='left')\n",
    "#          .merge(EXT_2,on='SK_ID_CURR',how='left')\n",
    "#          .merge(EXT_3,on='SK_ID_CURR',how='left')\n",
    "#         )\n",
    "# tr_te['EXT_SOURCE_1'] = tr_te['EXT_SOURCE_1'].combine_first(tr_te['EXT_1'])\n",
    "# tr_te['EXT_SOURCE_2'] = tr_te['EXT_SOURCE_2'].combine_first(tr_te['EXT_2'])\n",
    "# tr_te['EXT_SOURCE_3'] = tr_te['EXT_SOURCE_3'].combine_first(tr_te['EXT_3'])\n",
    "# tr_te.drop(['EXT_1','EXT_2','EXT_3'], axis=1, inplace=True)\n",
    "# print(tr_te.shape)\n",
    "\n",
    "# tr=tr_te.iloc[:tri,:].copy()\n",
    "# te=tr_te.iloc[tri:,:].copy()\n",
    "# print(tr.shape, te.shape)\n",
    "\n",
    "# del tr_EXT, te_EXT, tr_te_EXT, tr_te, EXT_1_null, EXT_2_null, EXT_3_null, EXT_1, EXT_2, EXT_3\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(tr_te): \n",
    "    tr_te.update(tr_te[['AMT_ANNUITY','OWN_CAR_AGE','DAYS_LAST_PHONE_CHANGE']].fillna(0))\n",
    "    tr_te['AMT_GOODS_PRICE'] = tr_te['AMT_GOODS_PRICE'].combine_first(tr_te['AMT_CREDIT'])\n",
    "    tr_te['CNT_FAM_MEMBERS'] = tr_te['CNT_FAM_MEMBERS'].combine_first(1+tr_te['CNT_CHILDREN'])\n",
    "    tr_te['HOUR_APPR_PROCESS_START'].fillna(tr_te['HOUR_APPR_PROCESS_START'].mode()[0], inplace=True)\n",
    "    tr_te['CODE_GENDER'].replace('XNA','F', inplace=True)\n",
    "    tr_te['DAYS_EMPLOYED'].replace(365243,np.nan, inplace=True)\n",
    "#     tr_te['DAYS_EMPLOYED'] = tr_te['DAYS_EMPLOYED'].combine_first((-2222.68 +(-0.00099*tr_te['AMT_INCOME_TOTAL'])))\n",
    "    #Credit_Bureau_Req - fill missing values\n",
    "    cred_bur_req = [col for col in tr_te.columns if 'AMT_REQ_CREDIT_BUREAU' in col]\n",
    "    tr_te.update(tr_te[cred_bur_req].fillna(-1))\n",
    "    del cred_bur_req\n",
    "    \n",
    "    #Cat Features - Transformation\n",
    "    tr_te['HOUSETYPE_MODE'] = np.where(~tr_te.HOUSETYPE_MODE.isin(['block of flats']), 'Others', tr_te.HOUSETYPE_MODE)\n",
    "    tr_te['NAME_TYPE_SUITE'] = np.where(tr_te.NAME_TYPE_SUITE.isin(['Other_A','Other_B']), 'Others', tr_te.NAME_TYPE_SUITE)\n",
    "    tr_te['NAME_INCOME_TYPE'] = np.where(tr_te.NAME_INCOME_TYPE.isin(['Pensioner','State servant']), 'SteadyInc-Pension-State', tr_te.NAME_INCOME_TYPE)\n",
    "    tr_te['NAME_HOUSING_TYPE'] = np.where(tr_te.NAME_HOUSING_TYPE.isin(['Rented apartment','With parents']), 'Not a Landlord', tr_te.NAME_HOUSING_TYPE)\n",
    "    tr_te['NAME_FAMILY_STATUS'] = np.where(tr_te.NAME_FAMILY_STATUS.isin(['Civil marriage','Single / not married']), 'Not_Religiously_Married', tr_te.NAME_FAMILY_STATUS)\n",
    "    tr_te['OCCUPATION_TYPE'] = np.where(tr_te.OCCUPATION_TYPE.isin(['Accountants','High skill tech staff','Core staff']), 'Reliable_Pay_Grade', tr_te.OCCUPATION_TYPE)\n",
    "    tr_te['OCCUPATION_TYPE'] = np.where(tr_te.OCCUPATION_TYPE.isin(['Cooking staff','Laborers','Drivers']), 'Low_Pay_Grade', tr_te.OCCUPATION_TYPE)\n",
    "    tr_te['FONDKAPREMONT_MODE'] = np.where(tr_te.FONDKAPREMONT_MODE.isin(['org spec account','reg oper account','reg oper spec account']), 'specified', 'not specified')\n",
    "    tr_te['WALLSMATERIAL_MODE'] = np.where(tr_te.WALLSMATERIAL_MODE.isin(['Panel','Stone, brick','Block','Monolithic']), 'Sturdy/Custom', tr_te.WALLSMATERIAL_MODE)\n",
    "    tr_te['WALLSMATERIAL_MODE'] = np.where(tr_te.WALLSMATERIAL_MODE.isin(['Wooden',np.nan]), 'Wooden/NotSpecified', tr_te.WALLSMATERIAL_MODE)\n",
    "    tr_te['NAME_EDUCATION_TYPE'] = np.where(tr_te.NAME_EDUCATION_TYPE.isin(['Secondary / secondary special','Lower secondary']), 'Secondary', tr_te.NAME_EDUCATION_TYPE)\n",
    "    tr_te['ORGANIZATION_TYPE'] = np.where(tr_te.ORGANIZATION_TYPE.isin(['Business Entity Type 3','Trade: type 3','Trade: type 7']), 'Biz', tr_te.ORGANIZATION_TYPE)\n",
    "    tr_te['ORGANIZATION_TYPE'] = np.where(tr_te.ORGANIZATION_TYPE.isin(['Agriculture','Construction']), 'Physical_Labor', tr_te.ORGANIZATION_TYPE)\n",
    "    tr_te['ORGANIZATION_TYPE'] = np.where(tr_te.ORGANIZATION_TYPE.isin(['Government','Military','Police','Security Ministries']), 'Govt_Service', tr_te.ORGANIZATION_TYPE)\n",
    "    tr_te['ORGANIZATION_TYPE'] = np.where(tr_te.ORGANIZATION_TYPE.isin(['School','University']), 'Academic', tr_te.ORGANIZATION_TYPE)\n",
    "    tr_te['ORGANIZATION_TYPE'] = np.where(tr_te.ORGANIZATION_TYPE.isin(['Industry: type 1','Industry: type 3']), 'Industry', tr_te.ORGANIZATION_TYPE)\n",
    "    \n",
    "    soc_cols = [col for col in tr_te.columns if 'SOCIAL_CIRCLE' in col]\n",
    "    tr_te[soc_cols] = tr_te[soc_cols].astype(float)\n",
    "    tr_te.update(tr_te[soc_cols].fillna(tr_te[soc_cols].median()))\n",
    "    return tr_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = preprocess(tr)\n",
    "te = preprocess(te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "AGGREGATION_RECIPIES  = [\n",
    "    (['CODE_GENDER', 'NAME_EDUCATION_TYPE'], [('AMT_ANNUITY', 'max'),\n",
    "                                              ('AMT_CREDIT', 'max'),\n",
    "                                              ('EXT_SOURCE_1', 'mean'),\n",
    "                                              ('EXT_SOURCE_2', 'mean'),\n",
    "                                              ('OWN_CAR_AGE', 'max')]),\n",
    "    (['CODE_GENDER', 'ORGANIZATION_TYPE'], [('AMT_ANNUITY', 'mean'),\n",
    "                                            ('AMT_INCOME_TOTAL', 'mean'),\n",
    "                                            ('DAYS_REGISTRATION', 'mean'),\n",
    "                                            ('EXT_SOURCE_1', 'mean')]),\n",
    "    (['CODE_GENDER', 'REG_CITY_NOT_WORK_CITY'], [('AMT_ANNUITY', 'mean'),\n",
    "                                                 ('CNT_CHILDREN', 'mean'),\n",
    "                                                 ('DAYS_ID_PUBLISH', 'mean')]),\n",
    "    (['CODE_GENDER', 'NAME_EDUCATION_TYPE', 'OCCUPATION_TYPE', 'REG_CITY_NOT_WORK_CITY'], [('EXT_SOURCE_1', 'mean'),\n",
    "                                                                                           ('EXT_SOURCE_2', 'mean')]),\n",
    "    (['NAME_EDUCATION_TYPE', 'OCCUPATION_TYPE'], [('AMT_CREDIT', 'mean'),\n",
    "                                                  ('AMT_REQ_CREDIT_BUREAU_YEAR', 'mean'),\n",
    "                                                  ('APARTMENTS_AVG', 'mean'),\n",
    "                                                  ('BASEMENTAREA_AVG', 'mean'),\n",
    "                                                  ('EXT_SOURCE_1', 'mean'),\n",
    "                                                  ('EXT_SOURCE_2', 'mean'),\n",
    "                                                  ('EXT_SOURCE_3', 'mean'),\n",
    "                                                  ('NONLIVINGAREA_AVG', 'mean'),\n",
    "                                                  ('OWN_CAR_AGE', 'mean'),\n",
    "                                                  ('YEARS_BUILD_AVG', 'mean')]),\n",
    "    (['NAME_EDUCATION_TYPE', 'OCCUPATION_TYPE', 'REG_CITY_NOT_WORK_CITY'], [('ELEVATORS_AVG', 'mean'),\n",
    "                                                                            ('EXT_SOURCE_1', 'mean')]),\n",
    "    (['OCCUPATION_TYPE'], [('AMT_ANNUITY', 'mean'),\n",
    "                           ('CNT_CHILDREN', 'mean'),\n",
    "                           ('CNT_FAM_MEMBERS', 'mean'),\n",
    "                           ('DAYS_BIRTH', 'mean'),\n",
    "                           ('DAYS_EMPLOYED', 'mean'),\n",
    "                           ('DAYS_ID_PUBLISH', 'mean'),\n",
    "                           ('DAYS_REGISTRATION', 'mean'),\n",
    "                           ('EXT_SOURCE_1', 'mean'),\n",
    "                           ('EXT_SOURCE_2', 'mean'),\n",
    "                           ('EXT_SOURCE_3', 'mean')]),                 \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform aggregation based on recipes\n",
    "tr = aggregate_and_find_diff(tr,AGGREGATION_RECIPIES)\n",
    "te = aggregate_and_find_diff(te,AGGREGATION_RECIPIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_te = tr.append(te, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe: 246.66 MB\n",
      "Memory usage after optimization: 197.74 MB\n",
      "Decreased by 19.8%\n"
     ]
    }
   ],
   "source": [
    "tr_te = reduce_mem_usage(tr_te, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Features from open solutions - 1\n",
    "tr_te['PAYMENT_RATE'] = tr_te['AMT_ANNUITY'] / tr_te['AMT_CREDIT']\n",
    "#Features from open solutions - 2\n",
    "tr_te['LONG_EMPLOYMENT'] = (tr_te['DAYS_EMPLOYED'] < -2000).astype(int)\n",
    "\n",
    "tr_te['CNT_NON_CHILD'] = tr_te['CNT_FAM_MEMBERS'] - tr_te['CNT_CHILDREN']\n",
    "tr_te['CHILD_TO_NON_CHILD_RATIO'] = tr_te['CNT_CHILDREN'] / tr_te['CNT_NON_CHILD']\n",
    "tr_te['INCOME_PER_NON_CHILD'] = tr_te['AMT_INCOME_TOTAL'] / tr_te['CNT_NON_CHILD']\n",
    "tr_te['CREDIT_TO_PERSON'] = tr_te['AMT_CREDIT'] / tr_te['CNT_FAM_MEMBERS']\n",
    "tr_te['CREDIT_PER_CHILD'] = tr_te['AMT_CREDIT'] / (1 + tr_te['CNT_CHILDREN'])\n",
    "tr_te['CREDIT_PER_NON_CHILD'] = tr_te['AMT_CREDIT'] / tr_te['CNT_NON_CHILD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating bins of hours\n",
    "bins = [0,11,15,24]\n",
    "tr_te['HOUR_APPR_PROCESS_START'] = pd.cut(tr_te['HOUR_APPR_PROCESS_START'],bins, labels=[\"morning\", \"noon\", \"evening\"], right=False)\n",
    "di = {\"MONDAY\" : 1,\"TUESDAY\": 1,\"WEDNESDAY\": 1,\"THURSDAY\": 1,\"FRIDAY\": 1,\n",
    "      \"SATURDAY\" : 0,\"SUNDAY\" : 0}\n",
    "tr_te['WEEKDAY_APPR_PROCESS_START'] = tr_te['WEEKDAY_APPR_PROCESS_START'].map(di).fillna(tr_te['WEEKDAY_APPR_PROCESS_START'])\n",
    "# Some simple new features (percentages)\n",
    "tr_te['DAYS_EMPLOYED_PERC'] = tr_te['DAYS_EMPLOYED'] / tr_te['DAYS_BIRTH']\n",
    "tr_te['INCOME_CREDIT_PERC'] = tr_te['AMT_INCOME_TOTAL'] / tr_te['AMT_CREDIT']\n",
    "tr_te['INCOME_PER_PERSON'] = tr_te['AMT_INCOME_TOTAL'] / tr_te['CNT_FAM_MEMBERS']\n",
    "tr_te['ANNUITY_INCOME_PERC'] = (tr_te['AMT_ANNUITY'] / (1+tr_te['AMT_INCOME_TOTAL'])).pow(1/2)\n",
    "tr_te['LOAN_INCOME_RATIO'] = tr_te['AMT_CREDIT'] / tr_te['AMT_INCOME_TOTAL']\n",
    "tr_te['ANNUITY_LENGTH'] = tr_te['AMT_CREDIT'] / (1 + tr_te['AMT_ANNUITY'])\n",
    "tr_te['CHILDREN_RATIO'] = tr_te['CNT_CHILDREN'] / tr_te['CNT_FAM_MEMBERS'] \n",
    "tr_te['CREDIT_TO_GOODS_RATIO'] = tr_te['AMT_CREDIT'] / tr_te['AMT_GOODS_PRICE']\n",
    "tr_te['INC_PER_CHLD'] = tr_te['AMT_INCOME_TOTAL'] / (1 + tr_te['CNT_CHILDREN'])\n",
    "tr_te['SOURCES_PROD'] = tr_te['EXT_SOURCE_1'] * tr_te['EXT_SOURCE_2'] * tr_te['EXT_SOURCE_3']\n",
    "tr_te['CAR_TO_BIRTH_RATIO'] = (365*tr_te['OWN_CAR_AGE']) / tr_te['DAYS_BIRTH']\n",
    "tr_te['CAR_TO_EMPLOY_RATIO'] = (365*tr_te['OWN_CAR_AGE']) / (-1 + tr_te['DAYS_EMPLOYED'])\n",
    "tr_te['PHONE_TO_BIRTH_RATIO'] = tr_te['DAYS_LAST_PHONE_CHANGE'] / tr_te['DAYS_BIRTH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [_f for _f in tr_te.columns if 'FLAG_DOC' in _f]\n",
    "live = [_f for _f in tr_te.columns if ('FLAG_' in _f) & ('FLAG_DOC' not in _f) & ('_FLAG_' not in _f)]\n",
    "inc_by_org = tr_te[['AMT_INCOME_TOTAL', 'ORGANIZATION_TYPE']].groupby('ORGANIZATION_TYPE').median()['AMT_INCOME_TOTAL']\n",
    "tr_te['NEW_DOC_IND_AVG'] = tr_te[docs].mean(axis=1)\n",
    "tr_te['NEW_DOC_IND_STD'] = tr_te[docs].std(axis=1)\n",
    "tr_te['NEW_DOC_IND_KURT'] = tr_te[docs].kurtosis(axis=1)\n",
    "tr_te['NEW_LIVE_IND_SUM'] = tr_te[live].sum(axis=1)\n",
    "tr_te['NEW_LIVE_IND_STD'] = tr_te[live].std(axis=1)\n",
    "tr_te['NEW_LIVE_IND_KURT'] = tr_te[live].kurtosis(axis=1)\n",
    "tr_te['NEW_INC_BY_ORG'] = tr_te['ORGANIZATION_TYPE'].map(inc_by_org)\n",
    "tr_te['NEW_EXT_SOURCES_MEAN'] = tr_te[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].mean(axis=1).astype('float')\n",
    "tr_te['NEW_SCORES_STD'] = tr_te[['EXT_SOURCE_1', 'EXT_SOURCE_2', 'EXT_SOURCE_3']].std(axis=1)\n",
    "tr_te['NEW_SCORES_STD'] = tr_te['NEW_SCORES_STD'].fillna(tr_te['NEW_SCORES_STD'].mean())\n",
    "tr_te['NEW_PHONE_TO_EMPLOY_RATIO'] = tr_te['DAYS_LAST_PHONE_CHANGE'] / (-1+tr_te['DAYS_EMPLOYED'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropcolumn=['FLAG_DOCUMENT_2','FLAG_DOCUMENT_4',\n",
    "    'FLAG_DOCUMENT_5','FLAG_DOCUMENT_6','FLAG_DOCUMENT_7',\n",
    "    'FLAG_DOCUMENT_8','FLAG_DOCUMENT_9','FLAG_DOCUMENT_10', \n",
    "    'FLAG_DOCUMENT_11','FLAG_DOCUMENT_12','FLAG_DOCUMENT_13',\n",
    "    'FLAG_DOCUMENT_14','FLAG_DOCUMENT_15','FLAG_DOCUMENT_16',\n",
    "    'FLAG_DOCUMENT_17','FLAG_DOCUMENT_18','FLAG_DOCUMENT_19',\n",
    "    'FLAG_DOCUMENT_20','FLAG_DOCUMENT_21']\n",
    "tr_te.drop(dropcolumn, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(356255, 1346) (305811, 303) (338857, 266) (339587, 237) (103558, 157) (337252, 103)\n"
     ]
    }
   ],
   "source": [
    "print (tr_te.shape, bureau_agg.shape, prev_agg.shape, ins_agg.shape, agg_cred_card_bal.shape, agg_pos_cash_balance.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %store ins_agg\n",
    "# %store agg_pos_cash_balance\n",
    "# %store bureau_agg\n",
    "# %store prev_agg\n",
    "# %store agg_cred_card_bal\n",
    "# %store tr_te\n",
    "# %store -z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Joining dataframes\n",
    "tr_te=(tr_te.drop(labels=['TARGET'],axis=1)\n",
    "         .merge(bureau_agg,on='SK_ID_CURR',how='left')\n",
    "         .merge(prev_agg,on='SK_ID_CURR',how='left')\n",
    "         .merge(ins_agg,on='SK_ID_CURR',how='left')\n",
    "         .merge(agg_cred_card_bal,on='SK_ID_CURR',how='left')\n",
    "         .merge(agg_pos_cash_balance,on='SK_ID_CURR',how='left')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_te['BURO_MISSING'] = tr_te['BURO_BB_MISSING'].isnull().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping ID Column\n",
    "tr_te.drop('SK_ID_CURR', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302\n"
     ]
    }
   ],
   "source": [
    "##Variables with LT 95% missing values\n",
    "tr_te_col = non_missing_col(tr_te)\n",
    "print(len(tr_te_col))\n",
    "tr_te = tr_te[tr_te_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(356255, 1286) (356255, 16)\n"
     ]
    }
   ],
   "source": [
    "#Filter columns into numerical & categorical\n",
    "df_num = tr_te.select_dtypes(include=[np.number]) #Numbers which are categories must be verified\n",
    "df_cat = tr_te.select_dtypes(exclude=[np.number])\n",
    "print (df_num.shape, df_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find Count of distinct values in categorical fields\n",
    "count = df_cat.apply(pd.Series.nunique)\n",
    "dstnct_cnt_col = pd.DataFrame(count, columns=['count'])\n",
    "# dstnct_cnt_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binary Variables(1 or 2)\n",
    "df_binary_col = dstnct_cnt_col.loc[dstnct_cnt_col['count'] <=2]\n",
    "df_binary_col = df_binary_col.index.values.tolist()\n",
    "df_binary = df_cat[df_binary_col]\n",
    "#Nominal Variables(2+)\n",
    "df_nominal_col = dstnct_cnt_col.loc[dstnct_cnt_col['count'] >2]\n",
    "df_nominal_col = df_nominal_col.index.values.tolist()\n",
    "df_nominal = df_cat[df_nominal_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: (356255, 1302)\n"
     ]
    }
   ],
   "source": [
    "tr_te[df_binary_col] = tr_te[df_binary_col].pipe(LabelEncoding_Cat)\n",
    "print ('dataset:', tr_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_cat_cols = df_nominal_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colsToInclude = ['HOUSETYPE_MODE_block of flats','NAME_TYPE_SUITE_Family','NAME_TYPE_SUITE_Unaccompanied',\n",
    "#                  'NAME_INCOME_TYPE_Commercial associate','NAME_INCOME_TYPE_Pensioner','NAME_INCOME_TYPE_State servant','NAME_INCOME_TYPE_Unemployed','NAME_INCOME_TYPE_Working',\n",
    "#                 'NAME_EDUCATION_TYPE_Higher education','NAME_EDUCATION_TYPE_Lower secondary','NAME_EDUCATION_TYPE_Secondary / secondary special',\n",
    "#                 'NAME_HOUSING_TYPE_House / apartment','NAME_HOUSING_TYPE_Rented apartment','NAME_HOUSING_TYPE_With parents',\n",
    "#                 'NAME_FAMILY_STATUS_Civil marriage','NAME_FAMILY_STATUS_Married','NAME_FAMILY_STATUS_Single / not married','NAME_FAMILY_STATUS_Widow',\n",
    "#                 'OCCUPATION_TYPE_Accountants','OCCUPATION_TYPE_Cooking staff','OCCUPATION_TYPE_Core staff','OCCUPATION_TYPE_Drivers','OCCUPATION_TYPE_High skill tech staff','OCCUPATION_TYPE_Laborers',\n",
    "#                 'FONDKAPREMONT_MODE_org spec account','FONDKAPREMONT_MODE_reg oper account','FONDKAPREMONT_MODE_reg oper spec account',\n",
    "#                 'WALLSMATERIAL_MODE_Block','WALLSMATERIAL_MODE_Monolithic','WALLSMATERIAL_MODE_Panel','WALLSMATERIAL_MODE_Stone, brick','WALLSMATERIAL_MODE_Wooden',\n",
    "#                 'ORGANIZATION_TYPE_Agriculture','ORGANIZATION_TYPE_Bank','ORGANIZATION_TYPE_Business Entity Type 3','ORGANIZATION_TYPE_Construction','ORGANIZATION_TYPE_Government','ORGANIZATION_TYPE_Industry: type 1',\n",
    "#                 'ORGANIZATION_TYPE_Industry: type 3','ORGANIZATION_TYPE_Industry: type 12','ORGANIZATION_TYPE_Medicine','ORGANIZATION_TYPE_Military','ORGANIZATION_TYPE_Police','ORGANIZATION_TYPE_Restaurant',\n",
    "#                 'ORGANIZATION_TYPE_Security','ORGANIZATION_TYPE_Security Ministries','ORGANIZATION_TYPE_Self-employed','ORGANIZATION_TYPE_Trade: type 3','ORGANIZATION_TYPE_Trade: type 7','ORGANIZATION_TYPE_Transport: type 3',\n",
    "#                 'ORGANIZATION_TYPE_School','ORGANIZATION_TYPE_University','ORGANIZATION_TYPE_XNA',\n",
    "#                 'HOUR_APPR_PROCESS_START_morning','HOUR_APPR_PROCESS_START_evening']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colsToInclude = ['NAME_TYPE_SUITE_Family','NAME_TYPE_SUITE_Unaccompanied','NAME_TYPE_SUITE_Others',\n",
    "                 'NAME_INCOME_TYPE_Commercial associate','NAME_INCOME_TYPE_SteadyInc-Pension-State','NAME_INCOME_TYPE_Unemployed','NAME_INCOME_TYPE_Working',\n",
    "                'NAME_EDUCATION_TYPE_Higher education','NAME_EDUCATION_TYPE_Secondary',\n",
    "                'NAME_HOUSING_TYPE_House / apartment','NAME_HOUSING_TYPE_Not a Landlord',\n",
    "                'NAME_FAMILY_STATUS_Married','NAME_FAMILY_STATUS_Not_Religiously_Married','NAME_FAMILY_STATUS_Widow',\n",
    "                'OCCUPATION_TYPE_Reliable_Pay_Grade','OCCUPATION_TYPE_Low_Pay_Grade',\n",
    "                'WALLSMATERIAL_MODE_Sturdy/Custom','WALLSMATERIAL_MODE_Wooden/NotSpecified',\n",
    "                'ORGANIZATION_TYPE_Physical_Labor','ORGANIZATION_TYPE_Bank','ORGANIZATION_TYPE_Biz','ORGANIZATION_TYPE_Govt_Service',\n",
    "                'ORGANIZATION_TYPE_Industry','ORGANIZATION_TYPE_Medicine','ORGANIZATION_TYPE_Restaurant',\n",
    "                'ORGANIZATION_TYPE_Security','ORGANIZATION_TYPE_Self-employed','ORGANIZATION_TYPE_Transport: type 3',\n",
    "                'ORGANIZATION_TYPE_Academic','ORGANIZATION_TYPE_XNA',\n",
    "                'HOUR_APPR_PROCESS_START_morning','HOUR_APPR_PROCESS_START_evening']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selected_dummy_cols(df,dummy_cat_cols,colsToInclude):\n",
    "    dummies = create_dummy_set(dummy_cat_cols,df[dummy_cat_cols])\n",
    "    dummies = dummies[colsToInclude]\n",
    "    df_n = pd.concat([df, dummies], axis=1, join_axes=[df.index])\n",
    "    df_n.drop(dummy_cat_cols,axis=1,inplace=True)\n",
    "    return df_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(356255, 13)\n",
      "(356255, 19)\n",
      "(356255, 26)\n",
      "(356255, 30)\n",
      "(356255, 35)\n",
      "(356255, 40)\n",
      "(356255, 57)\n",
      "(356255, 60)\n",
      "(356255, 66)\n",
      "(356255, 123)\n",
      "(356255, 125)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(356255, 1346)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr_te = selected_dummy_cols(tr_te,dummy_cat_cols,colsToInclude)\n",
    "tr_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change infinite values to Nan\n",
    "# tr_te.loc[tr_te.isin([np.inf, -np.inf]).any(axis='columns')].shape\n",
    "tr_te = tr_te.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Chi-SQ test on Categorical variables\n",
    "\n",
    "# def chisq_of_df_cols(df, c1, c2):\n",
    "#     groupsizes = df.groupby([c1, c2]).size()\n",
    "#     ctsum = groupsizes.unstack(c1)\n",
    "#     # fillna(0) is necessary to remove any NAs which will cause exceptions\n",
    "#     return(chi2_contingency(ctsum.fillna(0)))\n",
    "\n",
    "# def ChiSqPostHoc(df,col):\n",
    "#     dummies = pd.get_dummies(df[col])\n",
    "#     for series in dummies:\n",
    "#         nl = \"\\n\"\n",
    "#         crosstab = pd.crosstab(dummies[series], tr['TARGET'])\n",
    "#         print (crosstab, nl)\n",
    "#         chi2, p, dof, expected = chi2_contingency(crosstab.fillna(0))\n",
    "#         print ('Chi2 value:', chi2, nl, 'p-value:', p, nl, 'dof:', dof,nl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_df=df.iloc[:tri,:].copy()\n",
    "\n",
    "# chisq_of_df_cols(tr_df, 'HOUR_APPR_PROCESS_START', 'TARGET')\n",
    "\n",
    "# #Post hoc Chi-Sq testing - HOUR_APPR_PROCESS_START\n",
    "# ChiSqPostHoc(tr_df,'HOUR_APPR_PROCESS_START')\n",
    "\n",
    "# del tr_df\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Function to merge bin low count categories\n",
    "# def replace_under_top(df, col, n):\n",
    "#     a = df[col].value_counts()\n",
    "#     #get top n values of index\n",
    "#     vals = a[:n].index\n",
    "#     #assign columns back\n",
    "#     df[col] = df[col].where(df[col].isin(vals), 'other')\n",
    "#     #rename processes column\n",
    "#     df = df.rename(columns={col : col + '_new'})\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_te = replace_under_top(tr_te, 'ORGANIZATION_TYPE', 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for removing correlated variables\n",
    "# threshold = 0.9999999999999\n",
    "\n",
    "# # Absolute value correlation matrix\n",
    "# corr_matrix = df_tr.corr().abs()\n",
    "# # corr_matrix.head()\n",
    "\n",
    "# # Upper triangle of correlations\n",
    "# upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "# # upper.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Select columns with correlations above threshold\n",
    "# to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "# print('There are %d columns to remove.' % (len(to_drop)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def corr_check(col):\n",
    "#     return upper[col].loc[upper[col] > 0.9999].to_frame()\n",
    "# corr_check('LAST_50_INST_PAID_OVER_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_tr = tr.drop(columns = to_drop)\n",
    "# df_te = te.drop(columns = to_drop)\n",
    "\n",
    "# print('Training shape: ', df_tr.shape)\n",
    "# print('Testing shape: ', df_te.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df_tr.append(df_te)[df_tr.columns.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute missing values\n",
    "## Fill days columns with a value to distinguish\n",
    "days_cols = [col for col in prev_agg.columns if 'DAYS' in col if 'SK_ID_CURR' not in col]\n",
    "not_days_cols = [col for col in prev_agg.columns if 'DAYS' not in col if 'SK_ID_CURR' not in col]\n",
    "tr_te.update(tr_te[days_cols].fillna(999))\n",
    "## Fill others - amount and payment with 0\n",
    "# tr_te.update(tr_te[not_days_cols].fillna(0))\n",
    "del days_cols, not_days_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute missing values\n",
    "## Fill days columns with a value to distinguish\n",
    "days_cols = [col for col in bureau_agg.columns if 'DAY' in col if '_DIFF' not in col]\n",
    "tr_te.update(tr_te[days_cols].fillna(999))\n",
    "days_diff_cols = [col for col in bureau_agg.columns if '_DIFF' in col]\n",
    "tr_te.update(tr_te[days_diff_cols].fillna(-1))\n",
    "### Fill others - amount and payment with 0\n",
    "not_days_cols = [col for col in bureau_agg.columns if 'DAY' not in col if 'SK_ID_CURR' not in col]\n",
    "# tr_te.update(tr_te[not_days_cols].fillna(0))\n",
    "\n",
    "del days_cols, days_diff_cols,not_days_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a master dataframe for model\n",
    "df = pd.concat([tr_te, y], axis=1)\n",
    "df = reduce_mem_usage(df, verbose=True)\n",
    "\n",
    "print (df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'CC_NAME_CONTRACT_STATUS_Approved_mean', 'CC_NAME_CONTRACT_STATUS_Demand_mean', 'CC_NAME_CONTRACT_STATUS_Sent proposal_mean', 'CC_NAME_CONTRACT_STATUS_Signed_mean',\n",
    "to_drop = ['ACTIVE_ACT_LOAN_PERIOD_MEDIAN','CLOSED_AMT_CREDIT_SUM_OVERDUE_SUM','PREV_NAME_CONTRACT_TYPE_nan_MEAN','PREV_APP_DAYS_DECISION_FOR_LAST_1_CREDITS_MEAN','APPROVED_DAYS_LAST_DUE_MIN','APPROVED_DAYS_LAST_DUE_MAX','APPROVED_DAYS_LAST_DUE_MEAN','APPROVED_DAYS_TERMINATION_MAX','APPROVED_DAYS_TERMINATION_MEAN','APPROVED_DAYS_FIRST_DRAWING_MIN','APPROVED_DAYS_FIRST_DRAWING_MAX','APPROVED_DAYS_FIRST_DRAWING_MEAN','APPROVED_LOAN_PERIOD_MAX','APPROVED_LOAN_PERIOD_SUM','APPROVED_DAYS_FIRST_DUE_MIN','APPROVED_DAYS_FIRST_DUE_MAX','APPROVED_DAYS_FIRST_DUE_MEAN','APPROVED_DAYS_LAST_DUE_1ST_VERSION_MIN','APPROVED_DAYS_LAST_DUE_1ST_VERSION_MAX','APPROVED_DAYS_LAST_DUE_1ST_VERSION_MEAN','APPROVED_AMT_APPLICATION_MAX','APPROVED_AMT_APPLICATION_MEAN','CC_AMT_PAYMENT_CURRENT_count','CC_CNT_DRAWINGS_ATM_CURRENT_count','CC_CREDIT_LOAD_RATIO_count','CC_SK_DPD_BIN_count','CC_SK_DPD_DEF_BIN_count']\n",
    "df.drop(to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins_del = [col for col in ins_agg.columns if any(['INST_PAID_OVER_mean' in col, 'NUM_INSTALMENT_VERSION_' in col]) if 'INSTAL_NUM' not in col]\n",
    "df.drop(ins_del, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import resample\n",
    "\n",
    "# ## Revolving loans in training set\n",
    "# rev_tr = df.loc[df.TARGET.notnull()].loc[df.NAME_CONTRACT_TYPE==1]\n",
    "# print(rev_tr.shape)\n",
    "# ## Non Revolving loans in training set\n",
    "# df_other = df[~df.index.isin(rev_tr.index)]\n",
    "# print(df_other.shape)\n",
    "# ## Downsample revolving loans in training set\n",
    "# rev_tr = resample(rev_tr, replace=False,    # sample without replacement\n",
    "#                           n_samples=5000,     # to match minority class\n",
    "#                           random_state=123) # reproducible results\n",
    "# print(rev_tr.shape)\n",
    "# ## Append data\n",
    "# df = df_other.append(rev_tr)[df_other.columns.tolist()]\n",
    "# print(df.shape)\n",
    "# del rev_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_dim_col = ['APARTMENTS_AVG', 'BASEMENTAREA_AVG', 'YEARS_BEGINEXPLUATATION_AVG', 'YEARS_BUILD_AVG', 'COMMONAREA_AVG', 'ELEVATORS_AVG', 'ENTRANCES_AVG', 'FLOORSMAX_AVG', 'FLOORSMIN_AVG', 'LANDAREA_AVG', 'LIVINGAPARTMENTS_AVG', 'LIVINGAREA_AVG', 'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAREA_AVG', 'APARTMENTS_MODE', 'BASEMENTAREA_MODE', 'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BUILD_MODE', 'COMMONAREA_MODE', 'ELEVATORS_MODE', 'ENTRANCES_MODE', 'FLOORSMAX_MODE', 'FLOORSMIN_MODE', 'LANDAREA_MODE', 'LIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE', 'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAREA_MODE', 'APARTMENTS_MEDI', 'BASEMENTAREA_MEDI', 'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BUILD_MEDI', 'COMMONAREA_MEDI', 'ELEVATORS_MEDI', 'ENTRANCES_MEDI', 'FLOORSMAX_MEDI', 'FLOORSMIN_MEDI', 'LANDAREA_MEDI', 'LIVINGAPARTMENTS_MEDI', 'LIVINGAREA_MEDI', 'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAREA_MEDI', 'TOTALAREA_MODE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(app_dim_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_col = [col for col in agg_cred_card_bal.columns if 'CC' in col if 'LAST' not in col]\n",
    "# print (cc_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_appr_ref_col = [col for col in prev_agg.columns if any(['APPROVED' in col,'REFUSED' in col])]\n",
    "# print (prev_appr_ref_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_feats = [app_dim_col,prev_appr_ref_col,cc_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L = []\n",
    "# for x in df.columns: \n",
    "#         L.append(x) \n",
    "# ins_open_col = L[786:960]\n",
    "# pos_open_col = L[1102:1152]\n",
    "# open_last_col = ins_open_col + pos_open_col\n",
    "# print (open_last_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA function with normalization\n",
    "def doPCAwS(df_train, df_validation, df_test, n):\n",
    "    #Create index dataframe to join later\n",
    "    df_train_idx = df_train.index.values.tolist()\n",
    "    df_train_idx = pd.DataFrame(df_train_idx).rename(columns = {0 : 'idx'})\n",
    "    df_validation_idx = df_validation.index.values.tolist()\n",
    "    df_validation_idx = pd.DataFrame(df_validation_idx).rename(columns = {0 : 'idx'})\n",
    "    df_test_idx = df_test.index.values.tolist()\n",
    "    df_test_idx = pd.DataFrame(df_test_idx).rename(columns = {0 : 'idx'})\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df_train)\n",
    "#     print(scaler.mean_)\n",
    "    X_train = scaler.transform(df_train)\n",
    "    X_validation = scaler.transform(df_validation)\n",
    "    X_test = scaler.transform(df_test)\n",
    "    \n",
    "    #Calling PCA function and fitting\n",
    "    pca = PCA(n_components=n)\n",
    "    pca.fit(X_train)\n",
    "    #The amount of variance that each PC explains\n",
    "    var = pca.explained_variance_ratio_\n",
    "    #Cumulative Variance explains\n",
    "    cum_var=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "    print (cum_var)\n",
    "    #Transform into PCs\n",
    "    X_train_tfm = pca.transform(X_train)\n",
    "    df_train_pca = pd.DataFrame(X_train_tfm)\n",
    "    X_validation_tfm = pca.transform(X_validation)\n",
    "    df_validation_pca = pd.DataFrame(X_validation_tfm)\n",
    "    X_test_tfm = pca.transform(X_test)\n",
    "    df_test_pca = pd.DataFrame(X_test_tfm)\n",
    "    #Join Index with non-PCA columns\n",
    "    df_train_pca = df_train_idx.join(df_train_pca).set_index(['idx'])\n",
    "    df_validation_pca = df_validation_idx.join(df_validation_pca).set_index(['idx'])\n",
    "    df_test_pca = df_test_idx.join(df_test_pca).set_index(['idx'])\n",
    "    del df_train_pca.index.name, df_validation_pca.index.name, df_test_pca.index.name\n",
    "    return df_train_pca, df_validation_pca, df_test_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute missing values for PCA columns with mean\n",
    "def impute_PCA(df,col):\n",
    "    df[col] = df[col].fillna(df[col].mean())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_FeatureSet(df,feats,PCA_df):\n",
    "    df_n = df.drop(feats, axis=1)\n",
    "    df_n = pd.concat([df_n, PCA_df], axis=1)\n",
    "    return df_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_Master(train_x,valid_x,test_df,PCA_feats,num,prefix = 'PCA_PREFIX'):    \n",
    "    #########PCA operation and recover the dataset#####################\n",
    "    #Columns and Imputation\n",
    "    train_x, valid_x, test_df = impute_PCA(train_x,PCA_feats), impute_PCA(valid_x,PCA_feats), impute_PCA(test_df,PCA_feats)\n",
    "    #PCA\n",
    "    train_x_pca, valid_x_pca, test_x_pca = doPCAwS(train_x[PCA_feats], valid_x[PCA_feats], test_df[PCA_feats], num)\n",
    "    #Rename the PCA columns\n",
    "    train_x_pca, valid_x_pca, test_x_pca = train_x_pca.add_prefix(prefix), valid_x_pca.add_prefix(prefix), test_x_pca.add_prefix(prefix)\n",
    "        \n",
    "    #Replace the original columns with PCA columns \n",
    "    train_x_pca = PCA_FeatureSet(train_x,PCA_feats,train_x_pca)\n",
    "    valid_x_pca = PCA_FeatureSet(valid_x,PCA_feats,valid_x_pca)\n",
    "    test_x_pca = PCA_FeatureSet(test_df,PCA_feats,test_x_pca)\n",
    "        \n",
    "    #Updated list of columns to consider for test\n",
    "    feats_new = [f for f in train_x_pca.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "    ####################################################################\n",
    "    return train_x_pca, valid_x_pca, test_x_pca, feats_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM GBDT with KFold or Stratified KFold\n",
    "# Parameters from Bayesian Opt\n",
    "def kfold_lightgbm(df, PCA_feats, num_folds, stratified = False, debug= False):\n",
    "    # Divide in training/validation and test data\n",
    "    train_df = df[df['TARGET'].notnull()]\n",
    "#     train_df = train_df[:10000]\n",
    "    test_df = df[df['TARGET'].isnull()].copy()\n",
    "    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
    "    del df\n",
    "    gc.collect()\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits = num_folds, shuffle=True, random_state=47)\n",
    "    else:\n",
    "        folds = KFold(n_splits= num_folds, shuffle=True, random_state=47)\n",
    "    # Create arrays and dataframes to store results\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
    "        \n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
    "\n",
    "    ############# PCA operations ###################################\n",
    "        print (len(feats))\n",
    "        train_x_pca, valid_x_pca, test_x_pca, feats_new = PCA_Master(train_x, valid_x, test_df, PCA_feats[0], 15, prefix = 'PCA_APP_DIM_')\n",
    "#         print (len(feats_new))\n",
    "        train_x_pca, valid_x_pca, test_x_pca, feats_new = PCA_Master(train_x_pca,valid_x_pca,test_x_pca,PCA_feats[1], 33, prefix = 'PCA_PREV_APPREF_')\n",
    "#         print (len(feats_new))\n",
    "        train_x_pca, valid_x_pca, test_x_pca, feats_new = PCA_Master(train_x_pca,valid_x_pca,test_x_pca,PCA_feats[2], 55, prefix = 'PCA_CC_')\n",
    "#         print (len(feats_new))\n",
    "    ################ Null Importance ######################################    \n",
    "        null_feats = feats_new #Testing with all features\n",
    "        train_x_pca, valid_x_pca, test_x_pca = train_x_pca[null_feats], valid_x_pca[null_feats], test_x_pca[null_feats]\n",
    "        print(train_x_pca.shape, valid_x_pca.shape, test_x_pca.shape)\n",
    "        \n",
    "        \n",
    "        clf=LGBMClassifier(n_jobs=20,random_state=232323,n_estimators=8000,learning_rate=0.01,\n",
    "                            num_leaves=36,colsample_bytree=0.10442488,subsample=0.9290019,bagging_freq=1,\n",
    "                            max_depth=8,reg_alpha=4.99842044,reg_lambda=1.60494325,min_split_gain=0.0753679496,\n",
    "                            scale_pos_weight=2.398597, min_child_weight=47.4521998,silent=-1,verbose=-1)\n",
    "\n",
    "        clf.fit(train_x_pca, train_y, eval_set=[(train_x_pca, train_y), (valid_x_pca, valid_y)], eval_metric= 'auc', verbose= 1000, early_stopping_rounds= 200)\n",
    "\n",
    "        oof_preds[valid_idx] = clf.predict_proba(valid_x_pca, num_iteration=clf.best_iteration_)[:, 1]\n",
    "        sub_preds += clf.predict_proba(test_x_pca, num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = null_feats\n",
    "        fold_importance_df[\"gain_importance\"] = clf.booster_.feature_importance(importance_type='gain')\n",
    "        fold_importance_df[\"split_importance\"] = clf.feature_importances_\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n",
    "        del clf, train_x, train_y, valid_x, valid_y, train_x_pca, valid_x_pca\n",
    "        gc.collect()\n",
    "    \n",
    "    np.savetxt(dir+\"lgb_oof_preds.csv\", oof_preds, delimiter=\",\") \n",
    "\n",
    "    print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))\n",
    "    # Write submission file and plot feature importance\n",
    "    if not debug:\n",
    "        Submission=pd.read_csv(dir+\"sample_submission.csv\")\n",
    "        Submission['TARGET']=sub_preds.copy()\n",
    "        Submission.to_csv(dir+\"submission_LGB_Clf.csv\", index= False)\n",
    "    return feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LightGBM GBDT with KFold or Stratified KFold\n",
    "# # Parameters from Bayesian Opt\n",
    "# def kfold_lightgbm(df, PCA_feats, num_folds, stratified = False, debug= False):\n",
    "#     # Divide in training/validation and test data\n",
    "#     train_df = df[df['TARGET'].notnull()].copy()\n",
    "#     train_df['LOAN_WEIGHTS'] = np.where(train_df.NAME_CONTRACT_TYPE==1, 0.4, 1)\n",
    "# #     train_df = train_df[:10000]\n",
    "#     test_df = df[df['TARGET'].isnull()].copy()\n",
    "#     print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
    "#     del df\n",
    "#     gc.collect()\n",
    "#     # Cross validation model\n",
    "#     if stratified:\n",
    "#         folds = StratifiedKFold(n_splits = num_folds, shuffle=True, random_state=47)\n",
    "#     else:\n",
    "#         folds = KFold(n_splits= num_folds, shuffle=True, random_state=47)\n",
    "#     # Create arrays and dataframes to store results\n",
    "#     oof_preds = np.zeros(train_df.shape[0])\n",
    "#     sub_preds = np.zeros(test_df.shape[0])\n",
    "\n",
    "#     feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']] \n",
    "\n",
    "#     Dparam = {'objective' : 'binary',\n",
    "#           'boosting_type': 'gbdt',\n",
    "#           'weight_column': \"name:LOAN_WEIGHTS\",\n",
    "#           'random_state':23,\n",
    "#           'metric' : 'auc',\n",
    "#           'nthread' : 16,\n",
    "#           'shrinkage_rate':0.01,\n",
    "#           'max_depth':8,\n",
    "#           'min_child_weight':47.4521998,\n",
    "#           'scale_pos_weight':2.398597,\n",
    "#           'bagging_fraction':0.9290019,\n",
    "#           'feature_fraction':0.10442488,\n",
    "#           'bagging_freq' : 1,\n",
    "#           'lambda_l1':4.99842044,\n",
    "#           'lambda_l2':1.60494325,\n",
    "#           'num_leaves':36,\n",
    "#           'min_gain_to_split':0.0753679496} \n",
    "    \n",
    "#     for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
    "      \n",
    "#         train_x_pca, feats_new = PCA_Master_null(train_df, PCA_feats[0], 15, prefix = 'PCA_APP_DIM_')\n",
    "#         train_x_pca, feats_new = PCA_Master_null(train_x_pca, PCA_feats[1], 33, prefix = 'PCA_PREV_APPREF_')\n",
    "#         train_x_pca, feats_new = PCA_Master_null(train_x_pca, PCA_feats[2], 55, prefix = 'PCA_CC_')\n",
    "        \n",
    "#         test_x_pca, feats_new = PCA_Master_null(test_df, PCA_feats[0], 15, prefix = 'PCA_APP_DIM_')\n",
    "#         test_x_pca, feats_new = PCA_Master_null(test_x_pca, PCA_feats[1], 33, prefix = 'PCA_PREV_APPREF_')\n",
    "#         test_x_pca, feats_new = PCA_Master_null(test_x_pca, PCA_feats[2], 55, prefix = 'PCA_CC_')\n",
    "    \n",
    "#         dtrain = lgb.Dataset(train_x_pca[feats_new].iloc[train_idx], train_x_pca['TARGET'].iloc[train_idx])\n",
    "#         dval = lgb.Dataset(train_x_pca[feats_new].iloc[valid_idx], train_x_pca['TARGET'].iloc[valid_idx])\n",
    " \n",
    "#         m_gbm=lgb.train(params=Dparam,train_set=dtrain,num_boost_round=8000,verbose_eval=1000,early_stopping_rounds=200,valid_sets=[dtrain,dval],valid_names=['train','valid'])\n",
    "#         oof_preds[valid_idx] = m_gbm.predict(train_x_pca[feats_new].iloc[valid_idx], num_iteration=m_gbm.best_iteration)\n",
    "#         sub_preds += m_gbm.predict(test_x_pca , num_iteration=m_gbm.best_iteration) / folds.n_splits\n",
    "#         print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(train_x_pca['TARGET'].iloc[valid_idx],oof_preds[valid_idx])))\n",
    "#         del dtrain,dval\n",
    "#         gc.collect()\n",
    "    \n",
    "#     np.savetxt(dir+\"lgb_oof_preds.csv\", oof_preds, delimiter=\",\") \n",
    "\n",
    "#     print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))\n",
    "#     # Write submission file and plot feature importance\n",
    "#     if not debug:\n",
    "#         Submission=pd.read_csv(dir+\"sample_submission.csv\")\n",
    "#         Submission['TARGET']=sub_preds.copy()\n",
    "#         Submission.to_csv(dir+\"submission_LGB_Clf_All.csv\", index= False)\n",
    "\n",
    "#     return Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display/plot feature importance\n",
    "def display_importances(feature_importance_df_):\n",
    "    cols = feature_importance_df_[[\"feature\", \"gain_importance\"]].groupby(\"feature\").mean().sort_values(by=\"gain_importance\", ascending=False)[:50].index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"gain_importance\", y=\"feature\", data=best_features.sort_values(by=\"gain_importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "#     plt.savefig(dir+'lgbm_importances01.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_importances(feat_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run LightGBM model\n",
    "feat_importance = kfold_lightgbm(df, PCA_feats, num_folds = 5, stratified= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L = [[x,useless_feats.count(x)] for x in set(useless_feats)]\n",
    "# Ll = [l[0:1] for l in L if l[1] == 5]\n",
    "# Ll = [item for sublist in Ll for item in sublist]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
