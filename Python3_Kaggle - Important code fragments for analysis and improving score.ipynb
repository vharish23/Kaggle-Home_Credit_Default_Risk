{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Null importance for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://www.kaggle.com/ogrellier/feature-selection-with-null-importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA function with normalization\n",
    "def doPCAwS_null(df_train, n):\n",
    "    #Create index dataframe to join later\n",
    "    df_train_idx = df_train.index.values.tolist()\n",
    "    df_train_idx = pd.DataFrame(df_train_idx).rename(columns = {0 : 'idx'})\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df_train)\n",
    "#     print(scaler.mean_)\n",
    "    X_train = scaler.transform(df_train)\n",
    "\n",
    "    \n",
    "    #Calling PCA function and fitting\n",
    "    pca = PCA(n_components=n)\n",
    "    pca.fit(X_train)\n",
    "    #The amount of variance that each PC explains\n",
    "    var = pca.explained_variance_ratio_\n",
    "    #Cumulative Variance explains\n",
    "    cum_var=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "    print (cum_var)\n",
    "    #Transform into PCs\n",
    "    X_train_tfm = pca.transform(X_train)\n",
    "    df_train_pca = pd.DataFrame(X_train_tfm)\n",
    "\n",
    "    #Join Index with non-PCA columns\n",
    "    df_train_pca = df_train_idx.join(df_train_pca).set_index(['idx'])\n",
    "\n",
    "    del df_train_pca.index.name\n",
    "    return df_train_pca\n",
    "\n",
    "#Impute missing values for PCA columns with mean\n",
    "def impute_PCA(df,col):\n",
    "    df[col] = df[col].fillna(df[col].mean())\n",
    "    return df\n",
    "\n",
    "#Function to replace the original columns with PCA columns \n",
    "def PCA_FeatureSet(df,feats,PCA_df):\n",
    "    df_n = df.drop(feats, axis=1)\n",
    "    df_n = pd.concat([df_n, PCA_df], axis=1)\n",
    "    return df_n\n",
    "\n",
    "def PCA_Master_null(train_x, PCA_feats, num, prefix = 'PCA_PREFIX'):    \n",
    "    #########PCA operation and recover the dataset#####################\n",
    "    #Columns and Imputation\n",
    "    train_x = impute_PCA(train_x,PCA_feats)\n",
    "    #PCA\n",
    "    train_x_pca = doPCAwS_null(train_x[PCA_feats], num)\n",
    "    #Rename the PCA columns\n",
    "    train_x_pca = train_x_pca.add_prefix(prefix)\n",
    "        \n",
    "    #Replace the original columns with PCA columns \n",
    "    train_x_pca = PCA_FeatureSet(train_x, PCA_feats, train_x_pca)\n",
    "        \n",
    "    #Updated list of columns to consider for test\n",
    "    feats_new = [f for f in train_x_pca.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "    ####################################################################\n",
    "    return train_x_pca, feats_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implement PCA on training set\n",
    "train_x_pca, feats_new = PCA_Master_null(df_tr, PCA_feats[0], 15, prefix = 'PCA_APP_DIM_')\n",
    "train_x_pca, feats_new = PCA_Master_null(train_x_pca, PCA_feats[1], 33, prefix = 'PCA_PREV_APPREF_')\n",
    "df_tr, feats_new = PCA_Master_null(train_x_pca, PCA_feats[2], 55, prefix = 'PCA_CC_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df_tr\n",
    "data = pd.concat([data, y], axis=1)\n",
    "del df_tr\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Thanks to Olivier\n",
    "def get_feature_importances(data, shuffle, seed=None):\n",
    "    # Gather real features\n",
    "    train_features = [f for f in data if f not in ['TARGET', 'SK_ID_CURR']]\n",
    "    # Go over fold and keep track of CV score (train and valid) and feature importances\n",
    "    # Shuffle target if required\n",
    "    y = data['TARGET'].copy()\n",
    "    if shuffle:\n",
    "        # Here you could as well use a binomial distribution\n",
    "        y = data['TARGET'].copy().sample(frac=1.0)\n",
    "    # Fit LightGBM in RF mode, yes it's quicker than sklearn RandomForest\n",
    "    dtrain = lgb.Dataset(data[train_features], y, free_raw_data=False, silent=True)\n",
    "    lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'rf',\n",
    "        'subsample': 0.623,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'num_leaves': 127,\n",
    "        'max_depth': 8,\n",
    "        'seed': 123,\n",
    "        'bagging_freq': 1,\n",
    "        'n_jobs': 4\n",
    "    }\n",
    "    \n",
    "    # Fit the model\n",
    "    clf = lgb.train(params=lgb_params, train_set=dtrain, num_boost_round=200)\n",
    "    \n",
    "    # Get feature importances\n",
    "    imp_df = pd.DataFrame()\n",
    "    imp_df[\"feature\"] = list(train_features)\n",
    "    imp_df[\"importance_gain\"] = clf.feature_importance(importance_type='gain')\n",
    "    imp_df[\"importance_split\"] = clf.feature_importance(importance_type='split')\n",
    "    imp_df['trn_score'] = roc_auc_score(y, clf.predict(data[train_features]))\n",
    "    \n",
    "    return imp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed the unexpected randomness of this world\n",
    "np.random.seed(123)\n",
    "# Get the actual importance, i.e. without shuffling\n",
    "actual_imp_df = get_feature_importances(data=data, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_imp_df = pd.DataFrame()\n",
    "nb_runs = 80\n",
    "import time\n",
    "start = time.time()\n",
    "dsp = ''\n",
    "for i in range(nb_runs):\n",
    "    # Get current run importances\n",
    "    imp_df = get_feature_importances(data=data, shuffle=True)\n",
    "    imp_df['run'] = i + 1 \n",
    "    # Concat the latest importances with the old ones\n",
    "    null_imp_df = pd.concat([null_imp_df, imp_df], axis=0)\n",
    "    # Erase previous message\n",
    "    for l in range(len(dsp)):\n",
    "        print('\\b', end='', flush=True)\n",
    "    # Display current run and time used\n",
    "    spent = (time.time() - start) / 60\n",
    "    dsp = 'Done with %4d of %4d (Spent %5.1f min)' % (i + 1, nb_runs, spent)\n",
    "    print(dsp, end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the output of run\n",
    "null_imp_df.to_csv(dir+'null_importances_distribution_rf.csv')\n",
    "actual_imp_df.to_csv(dir+'actual_importances_ditribution_rf.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_distributions(actual_imp_df_, null_imp_df_, feature_):\n",
    "    plt.figure(figsize=(13, 6))\n",
    "    gs = gridspec.GridSpec(1, 2)\n",
    "    # Plot Split importances\n",
    "    ax = plt.subplot(gs[0, 0])\n",
    "    a = ax.hist(null_imp_df_.loc[null_imp_df_['feature'] == feature_, 'importance_split'].values, label='Null importances')\n",
    "    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_['feature'] == feature_, 'importance_split'].mean(), \n",
    "               ymin=0, ymax=np.max(a[0]), color='r',linewidth=10, label='Real Target')\n",
    "    ax.legend()\n",
    "    ax.set_title('Split Importance of %s' % feature_.upper(), fontweight='bold')\n",
    "    plt.xlabel('Null Importance (split) Distribution for %s ' % feature_.upper())\n",
    "    # Plot Gain importances\n",
    "    ax = plt.subplot(gs[0, 1])\n",
    "    a = ax.hist(null_imp_df_.loc[null_imp_df_['feature'] == feature_, 'importance_gain'].values, label='Null importances')\n",
    "    ax.vlines(x=actual_imp_df_.loc[actual_imp_df_['feature'] == feature_, 'importance_gain'].mean(), \n",
    "               ymin=0, ymax=np.max(a[0]), color='r',linewidth=10, label='Real Target')\n",
    "    ax.legend()\n",
    "    ax.set_title('Gain Importance of %s' % feature_.upper(), fontweight='bold')\n",
    "    plt.xlabel('Null Importance (gain) Distribution for %s ' % feature_.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display_distributions(actual_imp_df_=actual_imp_df, null_imp_df_=null_imp_df, feature_='EXT_SOURCE_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores = []\n",
    "for _f in actual_imp_df['feature'].unique():\n",
    "    f_null_imps_gain = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_gain'].values\n",
    "    f_act_imps_gain = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_gain'].mean()\n",
    "    gain_score = np.log(1e-10 + f_act_imps_gain / (1 + np.percentile(f_null_imps_gain, 75)))  # Avoid divide by zero\n",
    "    f_null_imps_split = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_split'].values\n",
    "    f_act_imps_split = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_split'].mean()\n",
    "    split_score = np.log(1e-10 + f_act_imps_split / (1 + np.percentile(f_null_imps_split, 75)))  # Avoid divide by zero\n",
    "    feature_scores.append((_f, split_score, gain_score))\n",
    "\n",
    "scores_df = pd.DataFrame(feature_scores, columns=['feature', 'split_score', 'gain_score'])\n",
    "\n",
    "plt.figure(figsize=(24, 24))\n",
    "gs = gridspec.GridSpec(1, 2)\n",
    "# Plot Split importances\n",
    "ax = plt.subplot(gs[0, 0])\n",
    "sns.barplot(x='split_score', y='feature', data=scores_df.sort_values('split_score', ascending=False).iloc[0:100], ax=ax)\n",
    "ax.set_title('Feature scores wrt split importances', fontweight='bold', fontsize=12)\n",
    "# Plot Gain importances\n",
    "ax = plt.subplot(gs[0, 1])\n",
    "sns.barplot(x='gain_score', y='feature', data=scores_df.sort_values('gain_score', ascending=False).iloc[0:100], ax=ax)\n",
    "ax.set_title('Feature scores wrt gain importances', fontweight='bold', fontsize=12)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_scores = []\n",
    "for _f in actual_imp_df['feature'].unique():\n",
    "    f_null_imps = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_gain'].values\n",
    "    f_act_imps = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_gain'].values\n",
    "    gain_score = 100 * (f_null_imps < f_act_imps).sum() / f_null_imps.size\n",
    "    f_null_imps = null_imp_df.loc[null_imp_df['feature'] == _f, 'importance_split'].values\n",
    "    f_act_imps = actual_imp_df.loc[actual_imp_df['feature'] == _f, 'importance_split'].values\n",
    "    split_score = 100 * (f_null_imps < f_act_imps).sum() / f_null_imps.size\n",
    "    correlation_scores.append((_f, split_score, gain_score))\n",
    "\n",
    "corr_scores_df = pd.DataFrame(correlation_scores, columns=['feature', 'split_score', 'gain_score'])\n",
    "\n",
    "fig = plt.figure(figsize=(24, 24))\n",
    "gs = gridspec.GridSpec(1, 2)\n",
    "# Plot Split importances\n",
    "ax = plt.subplot(gs[0, 0])\n",
    "sns.barplot(x='split_score', y='feature', data=corr_scores_df.sort_values('split_score', ascending=False).iloc[0:100], ax=ax)\n",
    "ax.set_title('Feature scores wrt split importances', fontweight='bold', fontsize=14)\n",
    "# Plot Gain importances\n",
    "ax = plt.subplot(gs[0, 1])\n",
    "sns.barplot(x='gain_score', y='feature', data=corr_scores_df.sort_values('gain_score', ascending=False).iloc[0:100], ax=ax)\n",
    "ax.set_title('Feature scores wrt gain importances', fontweight='bold', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.suptitle(\"Features' split and gain scores\", fontweight='bold', fontsize=16)\n",
    "fig.subplots_adjust(top=0.93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_feature_selection(df=None, train_features=None, cat_feats=None, target=None):\n",
    "    # Fit LightGBM \n",
    "    dtrain = lgb.Dataset(df[train_features], target, free_raw_data=False, silent=True)\n",
    "    lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': .1,\n",
    "        'num_leaves': 20,\n",
    "        'colsample_bytree': 0.9497036,\n",
    "        'subsample': 0.8715623,\n",
    "        'bagging_freq': 1,\n",
    "        'max_depth': -1,\n",
    "        'reg_alpha': 0.042,\n",
    "        'reg_lambda': 0.0735,\n",
    "        'min_split_gain': 0.0222415,\n",
    "        'min_child_weight': 65,\n",
    "        'subsample': 0.8,\n",
    "        'seed': 13,\n",
    "        'n_jobs': 4,\n",
    "        'metric': 'auc'\n",
    "    }\n",
    "    \n",
    "    # Fit the model\n",
    "    hist = lgb.cv(\n",
    "        params=lgb_params, \n",
    "        train_set=dtrain, \n",
    "        num_boost_round=2000,\n",
    "        nfold=5,\n",
    "        stratified=True,\n",
    "        shuffle=True,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose_eval=0,\n",
    "        seed=17\n",
    "    )\n",
    "    # Return the last mean / std values \n",
    "    return hist['auc-mean'][-1], hist['auc-stdv'][-1]\n",
    "\n",
    "# features = [f for f in data.columns if f not in ['SK_ID_CURR', 'TARGET']]\n",
    "# score_feature_selection(df=data[features], train_features=features, target=data['TARGET'])\n",
    "for threshold in [0, 20, 40, 60 , 80 , 90, 95, 99]:\n",
    "# for threshold in [0, 10, 20, 30 , 40, 50 ,60 , 70, 80 , 90, 95, 99]:\n",
    "    split_feats = [_f for _f, _score, _ in correlation_scores if _score >= threshold]\n",
    "    print(len(split_feats))\n",
    "#     split_cat_feats = [_f for _f, _score, _ in correlation_scores if (_score >= threshold) & (_f in categorical_feats)]\n",
    "    gain_feats = [_f for _f, _, _score in correlation_scores if _score >= threshold]\n",
    "    print(len(gain_feats))\n",
    "#     gain_cat_feats = [_f for _f, _, _score in correlation_scores if (_score >= threshold) & (_f in categorical_feats)]\n",
    "                                                                                             \n",
    "    print('Results for threshold %3d' % threshold)\n",
    "    split_results = score_feature_selection(df=data, train_features=split_feats, target=data['TARGET'])\n",
    "    print('\\t SPLIT : %.6f +/- %.6f' % (split_results[0], split_results[1]))\n",
    "    gain_results = score_feature_selection(df=data, train_features=gain_feats, target=data['TARGET'])\n",
    "    print('\\t GAIN  : %.6f +/- %.6f' % (gain_results[0], gain_results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_split_feats = scores_df.sort_values(by='split_score', ascending=False).head(700)\n",
    "sc_gain_feats = scores_df.sort_values(by='gain_score', ascending=False).head(700)\n",
    "sc_split_gain_feats = sc_split_feats['feature'].append(sc_gain_feats['feature'])\n",
    "sc_feats = sc_split_gain_feats.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_split_feats = corr_scores_df.sort_values(by='split_score', ascending=False).head(890)\n",
    "corr_gain_feats = corr_scores_df.sort_values(by='gain_score', ascending=False).head(995)\n",
    "corr_split_gain_feats = corr_split_feats['feature'].append(corr_gain_feats['feature'])\n",
    "corr_feats = corr_split_gain_feats.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feats = corr_scores_df.feature.unique().tolist() ### scores_df can also be used to get all features\n",
    "len(all_feats)\n",
    "null_feats = list(set([*corr_feats,*sc_feats]))\n",
    "len(null_feats)\n",
    "useless = list(set(all_feats) - set(null_feats))\n",
    "useless.sort()\n",
    "# useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SHAP (SHapley Additive exPlanations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://www.kaggle.com/alijs1/explaining-model-s-predictions\n",
    "### https://www.kaggle.com/slundberg/interpreting-a-lightgbm-model\n",
    "### https://www.kaggle.com/hmendonca/lightgbm-predictions-explained-with-shap-0-796"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lightgbm(df, num_boost_round=100, debug= False):\n",
    "    # Divide in training/validation and test data\n",
    "    train_df = df[df['TARGET'].notnull()].copy()\n",
    "    print(\"Starting LightGBM. Train shape: {}\".format(train_df.shape))\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "    \n",
    "    train_x_pca, feats_new = PCA_Master_null(train_df, PCA_feats[0], 15, prefix = 'PCA_APP_DIM_')\n",
    "    train_x_pca, feats_new = PCA_Master_null(train_x_pca, PCA_feats[1], 33, prefix = 'PCA_PREV_APPREF_')\n",
    "    train_df, feats_new = PCA_Master_null(train_x_pca, PCA_feats[2], 55, prefix = 'PCA_CC_')\n",
    "\n",
    "    params = {\n",
    "        'objective':'binary',\n",
    "        'metric':'auc',\n",
    "        'nthread':16,\n",
    "        'learning_rate':0.01,\n",
    "        'num_leaves':36,\n",
    "        'colsample_bytree':0.10442488,\n",
    "        'subsample':0.9290019,\n",
    "        'bagging_freq':1,\n",
    "        'max_depth':8,\n",
    "        'reg_alpha':4.99842044,\n",
    "        'reg_lambda':1.60494325,\n",
    "        'min_split_gain':0.0753679496,\n",
    "        'min_child_weight':47.4521998,\n",
    "        'scale_pos_weight': 2.398597,\n",
    "        'verbose':500\n",
    "    }\n",
    "    \n",
    "    train_x = lgb.Dataset(train_df[feats_new], train_df['TARGET'], silent=True)\n",
    "    clf = lgb.train(params, train_x, num_boost_round)\n",
    "    return clf, train_df[feats_new], train_df['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf, train_df, train_y = train_lightgbm(df, num_boost_round = 7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###high speed algorithm to compute SHAP values for LightGBM (and XGBoost and CatBoost) - use clf.booster_\n",
    "explainer = shap.TreeExplainer(clf.booster_)\n",
    "shap_values = explainer.shap_values(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Truth:', train_y[254551])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the first prediction's explanation\n",
    "shap.force_plot(explainer.expected_value, shap_values[254551,:], train_df.iloc[254551,:], link='logit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"NEW_EXT_SOURCES_MEAN\", shap_values, train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization - parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/sz8416/simple-bayesian-optimization-for-lightgbm\n",
    "# !pip3 install bayesian-optimization\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_parameter_opt_lgb(df, init_round=15, opt_round=25, n_folds=5, random_seed=6, n_estimators=10000, learning_rate=0.1, output_process=False):\n",
    "    # prepare data\n",
    "    df_train = df[df['TARGET'].notnull()]\n",
    "#     df_train = df_train[:10000]\n",
    "    # Target\n",
    "    y = df_train['TARGET'].copy()\n",
    "    feats = [f for f in df_train.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "    df_train = df_train[feats]\n",
    "    \n",
    "    train_x_pca, feats_new = PCA_Master_null(df_train, PCA_feats[0], 15, prefix = 'PCA_APP_DIM_')\n",
    "    print(len(feats_new))\n",
    "    train_x_pca, feats_new = PCA_Master_null(train_x_pca, PCA_feats[1], 33, prefix = 'PCA_PREV_APPREF_')\n",
    "    print(len(feats_new))\n",
    "    df_train, feats_new = PCA_Master_null(train_x_pca, PCA_feats[2], 55, prefix = 'PCA_CC_')\n",
    "    print(len(feats_new))\n",
    "    \n",
    "#     df_train = df_train[null_feats]\n",
    "#     print(df_train.shape)\n",
    "    \n",
    "    train_data = lgb.Dataset(data=df_train, label = y, free_raw_data=False)\n",
    "    # parameters\n",
    "    def lgb_eval(num_leaves, feature_fraction, bagging_fraction, max_depth, lambda_l1, lambda_l2, \n",
    "                 min_split_gain, min_child_weight,scale_pos_weight,min_data_in_leaf):\n",
    "        params = {'application':'binary','num_iterations': n_estimators,'learning_rate':learning_rate, \n",
    "                  'early_stopping_round':150, 'metric':'auc','n_jobs':16}\n",
    "        \n",
    "        params[\"num_leaves\"] = int(round(num_leaves))\n",
    "        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n",
    "        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n",
    "        params['max_depth'] = int(round(max_depth))\n",
    "        params['lambda_l1'] = max(lambda_l1, 0)\n",
    "        params['lambda_l2'] = max(lambda_l2, 0)\n",
    "        params['min_split_gain'] = min_split_gain\n",
    "        params['min_child_weight'] = min_child_weight\n",
    "        params['scale_pos_weight'] = scale_pos_weight\n",
    "        params['min_data_in_leaf'] = int(round(min_data_in_leaf))\n",
    "\n",
    "        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, \n",
    "                           verbose_eval = 500, metrics=['auc'])\n",
    "        return max(cv_result['auc-mean'])\n",
    "    # range \n",
    "    lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (20, 35),\n",
    "                                            'feature_fraction': (0.1, 0.3),\n",
    "                                            'bagging_fraction': (0.8, 1),\n",
    "                                            'max_depth': (4, 9),\n",
    "                                            'lambda_l1': (0, 5),\n",
    "                                            'lambda_l2': (0, 3),\n",
    "                                            'min_split_gain': (0.01, 0.1),\n",
    "                                            'scale_pos_weight': (2, 4),\n",
    "                                            'min_data_in_leaf': (20, 500),\n",
    "                                            'min_child_weight': (20, 50)}, random_state=0)\n",
    "    # optimize\n",
    "    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n",
    "    \n",
    "    # output optimization process\n",
    "    if output_process==True: lgbBO.points_to_csv(dir+\"bayes_opt_result.csv\")\n",
    "    \n",
    "    # return best parameters\n",
    "    return lgbBO.res['max']['max_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_params = bayes_parameter_opt_lgb(df, init_round=10, opt_round=15, n_folds=5, random_seed=123456, n_estimators=10000, learning_rate=0.02)\n",
    "print(opt_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Voting on OOF predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.ufunc size changed\")\n",
    "import numpy as np\n",
    "\n",
    "dir = '../Documents/JK/Home_Credit_Default_Risk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING Voting METHOD\n",
    "data = {}\n",
    "\n",
    "for path in glob.glob(\"../Documents/JK/Home_Credit_Default_Risk/Oof_files/*.csv\", recursive=True):\n",
    "    data[path[65:-4]] = pd.read_csv(path, header=None)\n",
    "\n",
    "oof_preds = pd.DataFrame(columns=data.keys())\n",
    "\n",
    "id_tgt = pd.read_csv('../Documents/JK/Home_Credit_Default_Risk/Oof_files/ID_TGT/id_target.csv', index_col=0).reset_index(drop=True)\n",
    "df = id_tgt.join(oof_preds)\n",
    "\n",
    "for key in data.keys():\n",
    "    df[key] = data[key]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['GP'] = df['GP'].replace(0.0, np.nan)\n",
    "m = df.iloc[:, 2:].mean(axis=1)\n",
    "for i, col in enumerate(df):\n",
    "    df.iloc[:, i] = df.iloc[:, i].fillna(m)\n",
    "    \n",
    "##df.loc[df.SK_ID_CURR.isin(['141289','144669','196708','319880'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sub = {}\n",
    "\n",
    "for path in glob.glob(\"../Documents/JK/Home_Credit_Default_Risk/Test/*.csv\", recursive=True):\n",
    "    data_sub[path[65:-4]] = pd.read_csv(path, header=0)\n",
    "\n",
    "sub_preds = pd.DataFrame(columns=data_sub.keys())\n",
    "\n",
    "id_test = pd.read_csv('../Documents/JK/Home_Credit_Default_Risk/Test/sample/sample_submission.csv')\n",
    "id_test.drop('TARGET',axis=1,inplace=True)\n",
    "df_test = id_test.join(sub_preds)\n",
    "\n",
    "for key in data_sub.keys():\n",
    "    df_test[key] = data_sub[key].TARGET\n",
    "    \n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression(random_state=1)\n",
    "clf2 = RandomForestClassifier(n_estimators = 500, random_state=1, warm_start = True)\n",
    "clf3 = GaussianNB()\n",
    "clf4 = KNeighborsClassifier(n_neighbors = 1000)\n",
    "clf5 = AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['TARGET','SK_ID_CURR'], axis=1)\n",
    "y = df['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclf = VotingClassifier(estimators=[('lr', clf1), ('gnb', clf3), ('knn', clf4),\n",
    "                                    ('ada', clf5)],\n",
    "#                                     weights=[1,1,1,1], \n",
    "                                    voting='soft')\n",
    "eclf = eclf.fit(X, y)\n",
    "eclf_preds = np.zeros(df.shape[0])\n",
    "eclf_preds = eclf.predict_proba(X)[:,1]\n",
    "# eclf_tfm = eclf.transform(X)\n",
    "\n",
    "print('Full AUC score %.6f' % roc_auc_score(y, eclf_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [clf1, clf3, clf4, clf5]\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "for model in models:\n",
    "    scores = cross_val_score(model, X, y, cv=5, scoring='roc_auc')\n",
    "    print(str(model)[:5], scores.mean(), scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test.drop(['SK_ID_CURR'], axis=1)\n",
    "eclf_sub_preds = np.zeros(df_test.shape[0])\n",
    "eclf_sub_preds = eclf.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission=pd.read_csv(dir+\"sample_submission.csv\")\n",
    "Submission['TARGET']=eclf_sub_preds.copy()\n",
    "Submission.to_csv(dir+\"submission_Voting.csv\", index= False)\n",
    "Submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blending final Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USING WEIGHTED AVERAGE RANK METHOD\n",
    "data = {}\n",
    "\n",
    "for path in glob.glob(dir+\"Blend/*.csv\", recursive=True):\n",
    "    data[path[47:-4]] = pd.read_csv(path)\n",
    "\n",
    "ranks = pd.DataFrame(columns=data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data.keys():\n",
    "    ranks[key] = data[key].TARGET.rank(method='min')\n",
    "ranks['Average'] = ranks.mean(axis=1)\n",
    "ranks['Scaled Rank'] = (ranks['Average'] - ranks['Average'].min()) / (ranks['Average'].max() - ranks['Average'].min())\n",
    "ranks.corr()[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [0.7,0.3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks['Score'] = ranks[['kaggle_rankavg_LB_top_805','submission_rankavg_blend_voting']].mul(weights).sum(1) / ranks.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_lb = pd.read_csv(dir+\"sample_submission.csv\")\n",
    "submission_lb['TARGET'] = ranks['Score']\n",
    "submission_lb.to_csv(dir+\"Blend/Blend_of_final.csv\", index=None)\n",
    "submission_lb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANK AVERAGING - ENSEMBLE GUIDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://github.com/MLWave/Kaggle-Ensemble-Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXT_SOURCE imputation - continuous values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM GBDT with KFold or Stratified KFold\n",
    "def kfold_lightgbm(df, Ext_col, PCA_feats, null_feats, stratified = False, debug= False):\n",
    "    # Divide in training/validation and test data\n",
    "    train_df = df[df[Ext_col].notnull()]\n",
    "#     train_df = train_df[0:10000]\n",
    "    test_df = df[df[Ext_col].isnull()].copy()\n",
    "    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
    "    del df\n",
    "    gc.collect()\n",
    "    \n",
    "    # Create arrays and dataframes to store results\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index',Ext_col]]\n",
    "    \n",
    "    \n",
    "    train_x, valid_x, train_y, valid_y = train_test_split(train_df[feats], train_df[Ext_col], test_size=0.2, random_state=420)\n",
    "\n",
    "    print(train_x.shape, valid_x.shape, train_y.shape,valid_y.shape)\n",
    "    ############# PCA operations ###################################\n",
    "    print (len(feats))\n",
    "    train_x_pca, valid_x_pca, test_x_pca, feats_new = PCA_Master(train_x, valid_x, test_df, PCA_feats[0], 15, prefix = 'PCA_APP_DIM_')\n",
    "    print (len(feats_new))\n",
    "    train_x_pca, valid_x_pca, test_x_pca, feats_new = PCA_Master(train_x_pca,valid_x_pca,test_x_pca,PCA_feats[1], 35, prefix = 'PCA_PREV_APPREF_')\n",
    "    print (len(feats_new))\n",
    "    train_x_pca, valid_x_pca, test_x_pca, feats_new = PCA_Master(train_x_pca,valid_x_pca,test_x_pca,PCA_feats[2], 55, prefix = 'PCA_CC_')\n",
    "    print (len(feats_new))\n",
    "    #################################################################   \n",
    "#     null_feats = [col for col in null_feats if col in feats_new]\n",
    "    \n",
    "#     train_x_pca, valid_x_pca, test_x_pca = train_x_pca[null_feats], valid_x_pca[null_feats], test_x_pca[null_feats]\n",
    "    print(train_x_pca.shape)\n",
    "\n",
    "    # create dataset for lightgbm\n",
    "    lgb_train = lgb.Dataset(train_x_pca, train_y)\n",
    "    lgb_eval = lgb.Dataset(valid_x_pca, valid_y, reference=lgb_train)\n",
    "    \n",
    "    watchlist = [lgb_train, lgb_eval]\n",
    "\n",
    "    # specify your configurations as a dict\n",
    "    params = {'task': 'train',\n",
    "                  'boosting_type': 'gbdt',\n",
    "                  'objective': 'regression',\n",
    "                  'metric': {'rmse','l2','l1'},\n",
    "                  'num_leaves': 25,\n",
    "                  'learning_rate': 0.01,\n",
    "                  'feature_fraction': 0.11384,\n",
    "                  'bagging_fraction': 0.893746,\n",
    "                  'bagging_freq': 1,\n",
    "                  'reg_alpha': 4.596,\n",
    "                  'reg_lambda': 2.836,\n",
    "                  'silent': -1,\n",
    "                  'verbose': -1\n",
    "              }\n",
    "\n",
    "    print('Start training...')\n",
    "    # train\n",
    "    gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=15000,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=300,\n",
    "                verbose_eval=1000)\n",
    "\n",
    "#       print('Save model...')\n",
    "#       # save model to file\n",
    "#       gbm.save_model('model.txt')\n",
    "\n",
    "    print('Start predicting...')\n",
    "    # predict\n",
    "    oof_preds = gbm.predict(valid_x_pca, num_iteration=gbm.best_iteration)\n",
    "#     sub_preds += gbm.predict(test_x_pca[null_feats], num_iteration=gbm.best_iteration) \n",
    "    sub_preds += gbm.predict(test_x_pca[feats_new], num_iteration=gbm.best_iteration)\n",
    "\n",
    "    # eval\n",
    "    print('The rmse of prediction is: ', mean_squared_error(valid_y, oof_preds) ** 0.5)\n",
    "    \n",
    "    del gbm, train_x, train_y, valid_x, valid_y\n",
    "    gc.collect()\n",
    "       \n",
    "    np.savetxt(Ext_col+\".csv\", sub_preds, delimiter=\",\")    \n",
    "\n",
    "    return feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputation\n",
    "feat_importance3 = kfold_lightgbm(df,'EXT_SOURCE_3', PCA_feats, null_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holdout -- First attempt -- not properly tested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = df[df['TARGET'].notnull()]\n",
    "# # train = train[:10000]\n",
    "# test = df[df['TARGET'].isnull()].copy()\n",
    "# test.drop('TARGET', axis=1, inplace=True)\n",
    "\n",
    "# from sklearn.cross_validation import KFold\n",
    "\n",
    "# # Some useful parameters which will come in handy later on\n",
    "# ntrain = train.shape[0]\n",
    "# ntest = test.shape[0]\n",
    "# SEED = 0 # for reproducibility\n",
    "# NFOLDS = 5 # set folds for out-of-fold prediction\n",
    "# kf = KFold(ntrain, n_folds = NFOLDS, random_state=SEED)\n",
    "\n",
    "# # Class to extend the Sklearn classifier\n",
    "# class SklearnHelper(object):\n",
    "#     def __init__(self, clf, seed=0, params=None):\n",
    "#         params['random_state'] = seed\n",
    "#         self.clf = clf(**params)\n",
    "\n",
    "#     def train(self, x_train, y_train):\n",
    "#         self.clf.fit(x_train, y_train)\n",
    "\n",
    "#     def predict(self, x):\n",
    "#         return self.clf.predict(x)\n",
    "    \n",
    "#     def fit(self,x,y):\n",
    "#         return self.clf.fit(x,y)\n",
    "    \n",
    "#     def feature_importances(self,x,y):\n",
    "#         print(self.clf.fit(x,y).feature_importances_)\n",
    "    \n",
    "# # Class to extend LightGBM classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_oof(clf, x_train, y_train, x_test):\n",
    "#     oof_train = np.zeros((ntrain,))\n",
    "#     oof_test = np.zeros((ntest,))\n",
    "#     oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "    \n",
    "#     print (len(oof_train), len(oof_test), np.shape(oof_test_skf))\n",
    "#     print (x_train.shape, y_train.shape, x_test.shape)\n",
    "\n",
    "#     for i, (train_index, test_index) in enumerate(kf):\n",
    "#         x_tr = x_train[train_index]\n",
    "#         y_tr = y_train[train_index]\n",
    "#         x_te = x_train[test_index]\n",
    "\n",
    "#         print (x_tr.shape, y_tr.shape, x_te.shape) \n",
    "        \n",
    "#         clf.train(x_tr, y_tr)\n",
    "        \n",
    "#         oof_train[test_index] = clf.predict(x_te)\n",
    "        \n",
    "#         print (oof_train.shape) \n",
    "        \n",
    "#         oof_test_skf[i, :] = clf.predict(x_test)\n",
    "\n",
    "#     oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "#     return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Put in our parameters for said classifiers\n",
    "# # Random Forest parameters\n",
    "# lgb_params = {'objective' : 'binary',\n",
    "#           'boosting_type': 'gbdt',\n",
    "#           'metric' : 'auc',\n",
    "#           'nthread' : 4,\n",
    "#           'shrinkage_rate':0.1,\n",
    "#           'max_depth':8,\n",
    "#           'min_child_weight':65,\n",
    "#           'bagging_fraction':0.9497036,\n",
    "#           'feature_fraction':0.8715623,\n",
    "#           'bagging_freq' : 1,\n",
    "# #           'max_bin':50,\n",
    "#           'lambda_l1':0.041545473,\n",
    "#           'lambda_l2':0.0735294,\n",
    "#           'num_leaves':20,\n",
    "# #           'min_data_in_leaf':50,\n",
    "#           'min_gain_to_split':0.0222415}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb = SklearnHelper(clf=LGBMClassifier, seed=SEED, params=lgb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\n",
    "# y_train = train['TARGET'].ravel()\n",
    "# train = train.drop(['TARGET'], axis=1)\n",
    "# x_train = train.values # Creates an array of the train data\n",
    "# x_test = test.values # Creats an array of the test data\n",
    "\n",
    "# print (train.shape, y_train.shape)\n",
    "# print (len(x_train), len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb_oof_train, lgb_oof_test = get_oof(lgb,x_train, y_train, x_test) # LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_predictions_train = pd.DataFrame( {'LightGBM': lgb_oof_train.ravel()})\n",
    "# base_predictions_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = np.concatenate(( lgb_oof_train), axis=1)\n",
    "# x_test = np.concatenate(( lgb_oof_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune and compare XGB, LightGBM, RF with Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/eikedehling/tune-and-compare-xgb-lightgbm-rf-with-hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
